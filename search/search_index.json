{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":"modelkit  <p> Python framework for production ML systems. </p> <p> </p> <p><code>modelkit</code> is a minimalist yet powerful MLOps library for Python, built for people who want to deploy ML models to production.</p> <p>It packs several features which make your go-to-production journey a breeze, and ensures that the same exact code will run in production, on your machine, or on data processing pipelines.</p>"},{"location":"#quickstart","title":"Quickstart","text":"<p><code>modelkit</code> provides a straightforward and consistent way to wrap your prediction  code in a <code>Model</code> class:</p> <pre><code>from modelkit import Model\n\nclass MyModel(Model):\n    def _predict(self, item):\n        # This is where your prediction logic goes\n        ...\n        return result\n</code></pre> <p>Be sure to check out our tutorials in the documentation.</p>"},{"location":"#features","title":"Features","text":"<p>Wrapping your prediction code in <code>modelkit</code> instantly gives acces to all features:</p> <ul> <li>fast Model predictions can be batched for speed (you define the batching logic) with minimal overhead.</li> <li>composable Models can depend on other models, and evaluate them however you need to</li> <li>extensible Models can rely on arbitrary supporting configurations files called assets hosted on local or cloud object stores</li> <li>type-safe Models' inputs and outputs can be validated by pydantic, you get type annotations for your predictions and can catch errors with static type analysis tools during development.</li> <li>async Models support async and sync prediction functions. <code>modelkit</code> supports calling async code from sync code so you don't have to suffer from partially async code.</li> <li>testable Models carry their own unit test cases, and unit testing fixtures are available for pytest</li> <li>fast to deploy Models can be served in a single CLI call using fastapi</li> </ul> <p>In addition, you will find that <code>modelkit</code> is:</p> <ul> <li>simple Use pip to install <code>modelkit</code>, it is just a Python library.</li> <li>robust Follow software development best practices: version and test all your configurations and artifacts.</li> <li>customizable Go beyond off-the-shelf models: custom processing, heuristics, business logic, different frameworks, etc.</li> <li>framework agnostic Bring your own framework to the table, and use whatever code or library you want. <code>modelkit</code> is not opinionated about how you build or train your models.</li> <li>organized Version and share you ML library and artifacts with others, as a Python package or as a service. Let others use and evaluate your models!</li> <li>fast to code Just write the prediction logic and that's it. No cumbersome pre or postprocessing logic, branching options, etc... The boilerplate code is minimal and sensible.</li> </ul>"},{"location":"#installation","title":"Installation","text":"<p>Install the latest stable release with <code>pip</code>:</p> <pre><code>pip install modelkit\n</code></pre> <p>Optional dependencies are available for remote storage providers (see documentation)</p> <p><code>modelkit &gt;= 0.1</code> is now be shipped with <code>pydantic 2</code>, bringing significant performance improvements \ud83c\udf89 \u26a1</p> <p>You can refer to the modelkit migration note  to ease the migration process!</p>"},{"location":"#community","title":"Community","text":"<p>Join our community on Discord to get support and leave feedback</p>"},{"location":"#local-install","title":"Local install","text":"<p>Contributors, if you want to install and test locally:</p> <pre><code># install\nmake setup\n\n# lint &amp; test\nmake tests\n</code></pre>"},{"location":"cli/","title":"<code>modelkit</code> CLI","text":""},{"location":"cli/#models-description","title":"Models description","text":""},{"location":"cli/#describe","title":"Describe","text":"<p>This CLI prints out all relevant information on a  given <code>modelkit</code> model repository: <pre><code>modelkit describe [PACKAGE] [--required-models ...]\n</code></pre></p>"},{"location":"cli/#assets-listing","title":"Assets listing","text":"<p>This CLI will show all necessary assets to run models <pre><code>modelkit list-assets [PACKAGE] [--required-models ...]\n</code></pre></p>"},{"location":"cli/#download-necessary-assets","title":"Download necessary assets","text":"<p>This CLI will download all necessary assets to run models to the current <code>MODELKIT_ASSETS_DIR</code> <pre><code>modelkit download-assets [PACKAGE] [--required-models ...]\n</code></pre></p> <p>Once this is done, you can run the models without enabling a storage provider.</p>"},{"location":"cli/#dependencies-graph","title":"Dependencies graph","text":"<p>This CLI will create a .DOT file with a graph of all models, their assets and model dependencies. <pre><code>modelkit dependencies-graph [PACKAGE] [--required-models ...]\n</code></pre> This requires graphviz for the graph layout. </p>"},{"location":"cli/#predictions","title":"Predictions","text":""},{"location":"cli/#batch","title":"batch","text":"<p>This CLI will treat a given JSONL file, with one item per line and write the output of a  model as another JSONL file, using multiple processes for speed</p> <pre><code>modelkit batch MODEL_NAME DATA_IN DATA_OUT [--models PACKAGE] [--processes N_PROCESSES] [--unordered]\n</code></pre> <p>Where: - <code>--models</code> to tell it where to find the model - <code>--processes</code> allows you to define the number of processes (defaults to all CPUs) - <code>--unordered</code> does not preserve the order of outputs (as with <code>imap_unordered</code>), which may be faster</p>"},{"location":"cli/#benchmarking","title":"Benchmarking","text":""},{"location":"cli/#memory-benchmark","title":"Memory benchmark","text":"<p>This CLI attempts to measure the memory consumption  of a set of <code>modelkit</code> models: <pre><code>modelkit memory [PACKAGE] [--required-models ...]\n</code></pre></p>"},{"location":"cli/#time","title":"Time","text":"<p>This CLI accepts a model and item, and will time the prediction <pre><code>modelkit time MODEL_NAME EXAMPLE_ITEM --models PACKAGE\n</code></pre></p>"},{"location":"cli/#serving","title":"Serving","text":"<p><code>modelkit</code> provides a single CLI to run a local <code>FastAPI</code> server with all loaded models mounted as endpoints: <pre><code>modelkit serve PACKAGE [--required-models ...]\n</code></pre> This is useful in order to inspect the swagger.</p> <p>Important</p> <p>Note that models whose payloads are not serializable will not be exposed, this is true in particular of numpy arrays</p>"},{"location":"cli/#assets-management","title":"Assets management","text":"<p>To list all assets: <pre><code>modelkit assets list\n</code></pre></p> <p>To create a new asset: <pre><code>modelkit assets new /path/to/asset asset_category/asset_name\n</code></pre></p> <p>To update an asset's minor version:</p> <pre><code>modelkit assets update /path/to/asset asset_category/asset_name\n</code></pre> <p>To push a new major version: <pre><code>modelkit assets update /path/to/asset asset_category/asset_name --bump-major\n</code></pre></p> <p>To retrieve a single asset <pre><code>modelkit assets fetch asset/spec [--download]\n</code></pre></p> <p>Use <code>--download</code> to force the re-download of the asset.</p>"},{"location":"cli/#tf-serving","title":"TF serving","text":"<p>To configure models from a package to be run in TF serving: <pre><code>modelkit tf-serving local-docker --models [PACKAGE]\n</code></pre></p> <p>This will write a configuration file with relative paths to the model files. This is meant to be used by mounting the <code>MODELKIT_ASSETS_DIR</code> in the container under the path <code>/config</code>.</p> <p>Other options include: - <code>local-process</code> To create a config file with absolute paths to the assets under <code>MODELKIT_ASSETS_DIR</code> - <code>remote</code> which will use whichever remote paths are found for the assets (i.e. as configured by the <code>MODELKIT_STORAGE_PROVIDER</code>)</p>"},{"location":"configuration/","title":"Configuration","text":""},{"location":"configuration/#environment","title":"Environment","text":"<p>In order to run/deploy <code>modelkit</code> endpoints, you need to provide it with the necessary environment variables, most of them required by <code>modelkit.assets</code> to retrieve assets from the remote object store:</p>"},{"location":"configuration/#general-modelkit-environment-variables","title":"General <code>modelkit</code> environment variables","text":"<p>The assets directory is required to know where to find assets</p> <ul> <li><code>MODELKIT_ASSETS_DIR</code>: the local directory in which assets will be   downloaded and cached. This needs to be a valid local directory.</li> </ul> <p>It is convenient to set a default value of a package in which <code>ModelLibrary</code> will look for models:</p> <ul> <li><code>MODELKIT_DEFAULT_PACKAGE</code> (default <code>None</code>). It has to be findable (on the <code>PYTHONPATH</code>)</li> </ul> <p>Lazy loading is useful when you want the models to be loaded only when they are actually used.</p> <ul> <li><code>MODELKIT_LAZY_LOADING</code> (defaults to <code>False</code>) toggles lazy loading mode for the <code>ModelLibrary</code></li> </ul> <p>Due to the implementation of cloud drivers, which are not pickable, the lazy driver mode is useful when you want  to use the ModelLibrary in conjunction with libraries using pickle: PySpark, multiprocessing etc.</p> <ul> <li><code>MODELKIT_LAZY_DRIVER</code> (defaults to <code>False</code>) toggles lazy mode for the <code>StorageProvider</code>'s drivers creation (boto3, gcs, azure)</li> </ul>"},{"location":"configuration/#storage-related-environment-variables","title":"Storage related environment variables","text":"<p>These variables are necessary to set a remote storage from which to retrieve assets. Refer to the storage provider documentation for more information for more information.</p> <ul> <li><code>MODELKIT_STORAGE_BUCKET</code> (default: unset): override storage container   where assets are retrieved from.</li> <li><code>MODELKIT_STORAGE_PREFIX</code> : the prefix under which objects are stored</li> <li><code>MODELKIT_STORAGE_PROVIDER</code> (default: <code>gcs</code>) the storage provider (does not have to be set)<ul> <li>for <code>MODELKIT_STORAGE_PROVIDER=gcs</code>, the variable <code>GOOGLE_APPLICATION_CREDENTIALS</code> need to be   pointing to a service account credentials JSON file (this is not necessary on dev   machines)</li> <li>for <code>MODELKIT_STORAGE_PROVIDER=s3</code>, you need to instantiate <code>AWS_PROFILE</code></li> <li>for <code>MODELKIT_STORAGE_PROVIDER=az</code>, you need to instantiate <code>AZURE_STORAGE_CONNECTION_STRING</code> with a connection string</li> </ul> </li> </ul>"},{"location":"configuration/#assets-versioning-related-environment-variable","title":"Assets versioning related environment variable","text":"<ul> <li><code>MODELKIT_ASSETS_VERSIONING_SYSTEM</code> will fix the assets versioning system. It can be <code>major_minor</code> or <code>simple_date</code></li> </ul>"},{"location":"configuration/#tf-serving-environment-variables","title":"TF serving environment variables","text":"<p>These environment variables can be used to parametrize tensorflow serving.</p> <ul> <li><code>MODELKIT_TF_SERVING_ENABLE</code> (default: <code>True</code>): Get tensorflow data from tensorflow server, instead of loading these data locally (if set to <code>False</code> you need to install tensorflow).<ul> <li><code>MODELKIT_TF_SERVING_HOST</code> (default: <code>localhost</code>): IP address of tensorflow server</li> <li><code>MODELKIT_TF_SERVING_PORT</code> (default: <code>8501</code>): Port of tensorflow server</li> <li><code>MODELKIT_TF_SERVING_MODE</code> (default: <code>rest</code>): <code>rest</code> to use REST protocol of tensorflow server (port 8501), <code>grpc</code> to use GRPC protocol (port 8500)</li> <li><code>TF_SERVING_TIMEOUT_S</code> (default: <code>60</code>): Timeout duration for tensorflow server calls</li> </ul> </li> </ul>"},{"location":"configuration/#cache-environment-variables","title":"Cache environment variables","text":"<p>These environment variables can be used to parametrize the caching.</p> <ul> <li><code>MODELKIT_CACHE_PROVIDER</code> (default: <code>None</code>) to use prediction caching</li> <li>if <code>MODELKIT_CACHE_PROVIDER=redis</code>, use an external redis instance for caching:<ul> <li><code>MODELKIT_CACHE_HOST</code> (default: <code>localhost</code>)</li> <li><code>MODELKIT_CACHE_PORT</code> (default: <code>6379</code>)</li> </ul> </li> <li>if <code>MODELKIT_CACHE_PROVIDER=native</code> use native caching (via cachetools):<ul> <li><code>MODELKIT_CACHE_IMPLEMENTATION</code> can be </li> <li><code>MODELKIT_CACHE_MAX_SIZE</code> size of the cache</li> </ul> </li> </ul>"},{"location":"migration/","title":"Modelkit 0.1 migration note","text":"<p>Modelkit relies on <code>pydantic</code> as part of its validation process.</p> <p>Modelkit 0.1 and onwards will be shipped with <code>pydantic 2</code>, which comes with significant performance improvements at the cost of breaking changes.</p> <p>Details on how to migrate to <code>pydantic 2</code> are available in the corresponding migration guide: https://docs.pydantic.dev/latest/migration/</p>"},{"location":"migration/#installation","title":"Installation","text":"<p>To install the brand new stable release of modelkit: <pre><code>pip install modelkit --upgrade\n</code></pre></p>"},{"location":"migration/#known-breaking-changes","title":"Known breaking changes","text":"<p>Some breaking changes are arising while upgrading to <code>pydantic 2</code> and the new <code>modelkit 0.1</code>. Here is a brief, rather exhaustive, list of the encountered issues or dropped features.</p>"},{"location":"migration/#drop-implicit-pydantic-model-conversion","title":"Drop: implicit pydantic model conversion","text":"<p>With <code>pydantic &lt; 2</code> and <code>modelkit &lt; 0.1</code>, the following pattern was authorized (even though not advised) due to implicit conversions between pydantic models:</p> <pre><code>import modelkit\nimport pydantic\nimport typing\n\nclass OutputItem(pydantic.BaseModel):\n    x: int\n\nclass AnotherOutputItem(pydantic.BaseModel):\n    x: int\n\nclass MyModel(modelkit.Model[int, OutputItem]):\n    def _predict(self, item):\n        return AnotherOutputItem(x=item)\n\nmodel = MyModel()\nmodel(1)  # raises!\n</code></pre> <p>This pattern is no longer allowed.</p> <p>However, here are the fixes: - directly build the right output <code>pydantic</code> Model (here: <code>OutputItem</code>) - directly use dicts to benefit from the dict to model conversion from <code>pydantic</code> and <code>modelkit</code> (or via <code>.model_dump()</code>)</p>"},{"location":"migration/#drop-model-validation-deactivation","title":"Drop: model validation deactivation","text":"<p>The <code>MODELKIT_ENABLE_VALIDATION</code> environment variable (or the <code>enable_validation</code> parameter of the <code>LibrarySettings</code>) which allowed one to deactivate validation if set to <code>False</code> was removed.</p> <p>This feature has worked for <code>pydantic &lt; 2</code> for rather simple <code>pydantic models</code> but not complex ones with nested structures (see: https://github.com/Cornerstone-OnDemand/modelkit/pull/8). However, it still is an open question in <code>pydantic 2</code>, whether to allow recursive construction of models without validation (see: https://github.com/pydantic/pydantic/issues/8084). Due to the fact <code>pydantic 2</code> brings heavy performance improvements, this feature has not been re-implemented.</p> <p>Fixes: None, just prepare to have your inputs / outputs validated :)</p>"},{"location":"migration/#development-workflows","title":"Development Workflows","text":"<p><code>modelkit 0.1</code> (and forward) changes will be pushed to the main branch.</p> <p>For projects that have not migrated, <code>modelkit 0.0</code> will continue to receive maintenance on the <code>v0.0-maintenance</code> branch. Releases on PyPI and manual tags will adhere to the usual process.</p> <p>To prevent your project from automatically upgrading to the new modelkit 0.1 upon its stable release, you can enforce an upper bound constraint in your requirements, e.g.: <code>modelkit&lt;0.1</code></p>"},{"location":"assets/assets_dir/","title":"Local assets dir","text":"<p>When used with a remote storage provider, <code>modelkit</code> will persist assets locally in the assets directory. This is very useful for development, whenever an asset is requested <code>modelkit</code> will first check if it is present before downloading the remote asset if necessary.</p> <p>Because assets are considered immutable, no checks are performed to verify that the objects have not been manually changed locally.</p>"},{"location":"assets/assets_dir/#assets-directory","title":"Assets directory","text":"<p>The local asset directory is found at <code>ASSETS_DIR</code>, although this can also be overrident when instantiating an <code>AssetsManager</code>.</p> <p>Each asset's name is splitted along the path separators as directories, and  version information is added.</p> <p>For example, we have pushed to the remote store two assets: a directory to <code>some/directory/asset</code> and a file to <code>some/asset</code>. After retrieving them to the <code>assets_dir</code>, it will look like this:</p> <pre><code>ASSETS_DIR\n\u2514\u2500\u2500 some\n|   \u251c\u2500\u2500 asset\n|   \u2502\u00a0\u00a0 \u251c\u2500\u2500 0.0 # &lt;- the file content\n|   \u2502\u00a0\u00a0 \u251c\u2500\u2500 .0.0.SUCCESS #\u00a0hidden file indicating download success\n|   \u2502\u00a0\u00a0 \u251c\u2500\u2500 0.1\n|   \u2502\u00a0\u00a0 |   ...\n|   \u251c\u2500\u2500 directory\n|   \u2502\u00a0\u00a0 \u251c\u2500\u2500 asset\n|   \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 0.0\n|   \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 |   \u251c\u2500\u2500 .SUCCESS #\u00a0hidden file indicating download success\n|   \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 |   \u251c\u2500\u2500 content0  &lt;- the directory contents\n|   \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 |   \u251c\u2500\u2500 content2\n|   \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 |   |   ...\n|   \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 0.1\n|   \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 |   |   ...\n|    ...\n</code></pre> <p>Note</p> <p>All previous versions of the assets are kept locally. It is safe, however to delete local copies of the assets manually to save space.</p> <p>For directory assets, delete the version directory. For file assets, do not forget to delete the <code>.version.SUCCESS</code> file too.</p> <p>To retrieve the assets path, refer to it via its asset specification:</p> <pre><code>mng = AssetsManager(assets_dir=assets_dir)\nmng.fetch_asset(\"some/asset:0.0\") # will point to `ASSETS_DIR/some/asset/0.0`\nmng.fetch_asset(\"some/directory/asset:0.1\") # will point to `ASSETS_DIR/some/asset/0.1`\nmng.fetch_asset(\"some/directory/asset:0.1[content0]\") # will point to `ASSETS_DIR/some/asset/0.1/content0`\n</code></pre>"},{"location":"assets/assets_dir/#version-resolution","title":"Version resolution","text":"<p>When an asset is requested with the <code>version</code> not fully specified, <code>modelkit</code> may need to consult the remote storage to find the latest version. As a result, <code>modelkit</code>'s asset manager will, in this context, have a different behavior depending on whether a remote storage provider is parametrized:</p> <ul> <li>Without a remote store find the latest version of the asset available in the <code>ASSETS_DIR</code></li> <li>With a remote store contact the remote store to find the latest version, check whether it is present locally. If it is, use the local version, otherwise download the latest version.</li> </ul> <p>This has an important consequence, which is that unpinned assets will always require network calls when being fetched, although they may already be present. For all production purposes, you should pin your asset versions.</p>"},{"location":"assets/environment/","title":"Environment","text":""},{"location":"assets/environment/#model-library","title":"Model library","text":"<p>The only necessary environment variable to set to use <code>modelkit</code> is the assets directory <code>MODELKIT_ASSETS_DIR</code> it has to be a valid local directory.</p>"},{"location":"assets/environment/#assetsmanager-settings","title":"AssetsManager settings","text":"<p>The parameters necessary to instantiate an <code>AssetsManager</code> can all be read from environment variables, or provided when initializing the <code>AssetsManager</code>.</p> Environment variable Default value Parameter Notes <code>MODELKIT_STORAGE_PROVIDER</code> <code>gcs</code> <code>storage_provider</code> <code>gcs</code> (default), <code>s3</code>, <code>az</code> or <code>local</code> <code>MODELKIT_STORAGE_BUCKET</code> None <code>bucket</code> Bucket in which data is stored <code>MODELKIT_STORAGE_PREFIX</code> <code>modelkit-assets</code> <code>prefix</code> Objects prefix <code>MODELKIT_STORAGE_TIMEOUT_S</code> <code>300</code> <code>timeout_s</code> max time when retrying storage downloads <code>MODELKIT_ASSETS_TIMEOUT_S</code> <code>10</code> <code>timeout</code> file lock timeout when downloading assets <p>More settings can be passed in order to configure the driver itself, see the storage provider documentation for more information</p>"},{"location":"assets/managing_assets/","title":"Managing assets","text":"<p>This section describes how to push assets either manually using CLI or programmatically.</p>"},{"location":"assets/managing_assets/#asset-maintenance-actions","title":"Asset maintenance actions","text":"<p>Since assets are immutable, there are only two actions one can take to affect the remotely stored assets.</p> <ul> <li>update an existing asset: If the asset name already exists remotely, the appropriate action is to update it.</li> <li>create a new asset: If this is the first time that this asset asset name  is created, the correct action is the create it.</li> </ul> <p><code>modelkit</code> does not offer ways to delete or replace assets.</p>"},{"location":"assets/managing_assets/#maintaining-assets-with-cli","title":"Maintaining assets with CLI","text":"<p><code>modelkit</code> implements CLIs to ease the maintenance of remote assets.</p> <p>Make sure the storage provider is properly setup with environment variables (see here).</p>"},{"location":"assets/managing_assets/#create-a-new-asset","title":"Create a new asset","text":"<p>To create a new asset:</p> <pre><code>modelkit assets new /path/to/asset/locally asset_name\n</code></pre> <p>After prompting you for confirmation, it will create a remote asset with initial version.</p> <p>(e.g. <code>0.0</code> for default major/minor versioning system, or the current date for \"simple date\" system)</p>"},{"location":"assets/managing_assets/#update-an-asset","title":"Update an asset","text":"<p>Use <code>modelkit assets update</code> to update an existing asset using a local file or directory at <code>/local/asset/path</code> under a new version according to the version system.</p>"},{"location":"assets/managing_assets/#majorminor-versioning-system","title":"Major/minor versioning system","text":"<ul> <li>Bump the minor version</li> </ul> <p>Assuming <code>name</code> has versions <code>0.1</code>, <code>1.1</code>, running   <pre><code>modelkit assets update /local/asset/path name\n</code></pre>   will add a version <code>1.2</code></p> <ul> <li>Bump the major version</li> </ul> <p>Assuming <code>name</code> has versions <code>0.1</code>, <code>1.0</code>, running</p> <pre><code>modelkit assets update /local/asset/path name --bump-major\n</code></pre> <p>After prompting your for confirmation, it will add a version <code>2.0</code></p> <ul> <li>Bump the minor version of an older asset</li> </ul> <p>Assuming <code>name</code> has versions <code>0.1</code>, <code>1.0</code>, running</p> <p><pre><code>modelkit assets update /local/asset/path name:0\n</code></pre>   will add a version <code>0.2</code></p>"},{"location":"assets/managing_assets/#simple-date-system","title":"Simple date system","text":"<p><code>modelkit assets update /local/asset/path name</code> will bump a new version equivalent to   the current UTC date in iso format <code>YYYY-MM-DDThh-mm-ssZ</code></p>"},{"location":"assets/managing_assets/#listing-remote-assets","title":"Listing remote assets","text":"<p>A CLI is also available to list remote assets in a given bucket:</p> <pre><code>modelkit assets list\n</code></pre>"},{"location":"assets/managing_assets/#maintaining-assets-programmatically","title":"Maintaining assets programmatically","text":"<p>First, instantiate an <code>AssetsManager</code> pointing to the desired <code>bucket</code>, possibly changing the <code>prefix</code>, storage method, etc.:</p> <pre><code>from modelkit.assets.remote import StorageProvider\n\nassets_store = StorageProvider()\n</code></pre>"},{"location":"assets/managing_assets/#create-a-new-asset_1","title":"Create a new asset","text":"<p>Assuming the asset is locally present at <code>asset_path</code> (either a file or a directory), create the remote asset <code>name:initial_version</code> as follows:</p> <pre><code>assets_store.new(asset_path, initial_version, name)\n</code></pre> <p>where <code>initial_version</code> is obtained using the <code>get_initial_version</code> method of your <code>AssetsVersioningSystem</code> system</p> <p>Important</p> <p>Creating new assets programmatically is possible, even though it is not considered a good practice. Using the CLI is the prefered and safest way to manage assets.</p>"},{"location":"assets/managing_assets/#update-the-asset","title":"Update the asset","text":"<p>Assuming the asset is locally present at <code>asset_path</code> (either a file or a directory), update the remote asset <code>name</code> as follows:</p> <pre><code>assets_store.update(\n  asset_path,\n  name,\n  version,\n)\n</code></pre> <p>where version is the new version which can be updated using the <code>increment_version</code> method of your <code>AssetsVersioningSystem</code> system</p>"},{"location":"assets/remote_assets/","title":"Remote assets","text":"<p>Remote assets allow you to share the necessary files and folders necessary to run your models with other members of your team, as well as with production services.</p> <p>In addition <code>modelkit</code> helps with the versioning of these files, and the management of your local developer copies.</p>"},{"location":"assets/remote_assets/#remote-asset-properties","title":"Remote asset properties","text":"<p>For <code>modelkit</code> remote assets are immutable, and their source of truth has to be a single remote object store. These have the following advantages:</p> <ul> <li>auditing it is possible to know which asset was used when by a production service, and pushed by whom</li> <li>reverting it is always possible to revert to an older version of a <code>Model</code> because assets will be present and cannot be modified.</li> <li>loss of data local machines can lose all of their data, assets will always be available from the remote store</li> <li>reproducibility the code running on your local machine is guaranteed to use the same artifacts and code as the one running in production</li> </ul> <p>Although these come at a cost, <code>modelkit</code> helps you manage, update, and create new assets.</p> <p>It also helps with maintaining your local copies of the assets to make development quicker (in the assets directory).</p>"},{"location":"assets/remote_assets/#model-with-remote-asset","title":"Model with remote asset","text":"<p>Using a remote asset in a <code>Model</code> is exactly the same thing as using a local one and using <code>_load</code> as we have seen before.</p> <p>We add a valid remote asset specification as a key in the configuration, <code>modelkit</code> will make sure to retrieve it before the <code>Model</code> is instantiated:</p> <pre><code>class ModelWithAsset(Model):\n    CONFIGURATIONS = {\n        \"model_with_asset\": {\"asset\": \"test/asset:1\"} # meaning \"version 1 of test/asset\"\n    }\n</code></pre> <p>Assuming that you have parametrized a GCS object store, this will cause the <code>ModelLibrary</code> to:</p> <ul> <li>download the objects at <code>gs://some-bucket-name/assets/test/1/yolo/*</code> locally (which storage provider and bucket is used depends on your configuration)</li> <li>write the files to the assets directory (controlled by <code>ASSETS_DIR</code>)</li> <li>set the <code>Model.asset_path</code> attribute accordingly.</li> </ul> <p>As a result, you can still write your own arbitrary <code>_load</code> logic, with confidence that the asset will actually be here</p> <pre><code>class ModelWithAsset(Model):\n    def _load(self):\n        # For example, here, the asset is a BZ2 compressed JSON file\n        with bz2.BZ2File(self.asset_path, \"rb\") as f:\n            self.data_structure = pickle.load(f) # loads {\"response\": \"Hello World!\"}\n\n    def _predict(self, item, **kwargs):\n        return self.data_structure[\"response\"] # returns \"Hello World!\"\n</code></pre>"},{"location":"assets/remote_assets/#asset-specification","title":"Asset specification","text":"<p>An asset specification string follows the convention:</p> <pre><code>name/of/asset/object:version[/asset/subobject]\n</code></pre> <p>Where:</p> <ul> <li>the name of the asset has to be a valid object store name (using <code>/</code> as a prefix separator)</li> <li><code>version</code> follows a semantic versioning system: (by default <code>major.minor</code> (e.g. <code>1.2</code>))</li> <li><code>[/asset/subobject]</code> optionally allows one to refer directly to a `sub object</li> </ul>"},{"location":"assets/remote_assets/#version-resolution","title":"Version resolution","text":"<p>Whenever a version is not completely set, the missing information is resolved to the latest version. For example:</p> <ul> <li><code>name/of/asset/object</code> is resolved to the latest version altogether</li> </ul> <p>Some versioning system can support partial version setting</p> <p>Example for major/minor system :</p> <ul> <li><code>name/of/asset/object:1</code> is resolved to the latest minor version <code>1.*</code></li> </ul>"},{"location":"assets/retrieving_assets/","title":"Retrieving assets","text":"<p>It is possible to access assets programmatically from their asset specification</p> <p>Once you have correctly configured your environment and storage provider:</p> <pre><code>from modelkit.assets.manager import AssetsManager\n\nmng = AssetsManager()\nasset_path = mng.fetch_asset(\"asset_category/asset_name:version[sub/part]\")\n\nwith open(asset_path, \"r\") as f:\n    # do something with the asset\n    ...\n</code></pre> <p>By default, <code>AssetsManager.fetch_asset</code> only returns the path to the locally downloaded asset, but it can return more information about the fetched asset if provided with the <code>return_info=True</code>.</p> <p>In this case it returns a dictionary with:</p> <pre><code>{\n    \"path\": \"/local/path/to/asset\",\n    \"from_cache\": True or False, # whether the asset was pulled from cache,\n    \"version\": \"returned asset version\", # the asset version\n    # These are present only when the asset was\n    # downloaded from the remote store:\n    \"meta\": {}, #\u00a0contents of the meta JSON object \n    # remote object names\n    \"object_name\": \"remote object name\", \n    \"meta_object_name\": \"remote meta object name\",\n    \"versions_object_name\": \"remote version object name\"\n}\n</code></pre>"},{"location":"assets/storage_provider/","title":"Storage provider","text":"<p>In order to take advantage of remote asset storage, you have to configure your environment to use the right storage provider.</p> <p>This is generally done by means of environment variables, and currently supports object stores on S3 (e.g. AWS, or minio) or GCS.</p> <p>The first thing you will need is a local directory in which assets will be retrieved and stored. This is best set in an environment variable <code>MODELKIT_ASSETS_DIR</code> which has to point to an existing directory.</p>"},{"location":"assets/storage_provider/#remote-storage-paths","title":"Remote storage paths","text":"<p>You will need a remote object store, as identified by a bucket, and <code>modelkit</code> will store all objects under a given prefix. </p> <p>These are controlled by the following environment variables</p> <ul> <li><code>MODELKIT_STORAGE_BUCKET</code> the name of the buket</li> <li><code>MODELKIT_STORAGE_PREFIX</code> the prefix of all <code>modelkit</code> objects in the bucket</li> </ul>"},{"location":"assets/storage_provider/#permissions","title":"Permissions","text":"<p>You will need to have credentials present with permissions.</p>"},{"location":"assets/storage_provider/#at-runtime","title":"At runtime","text":"<p>This is typically the case of running services, they need read access to all the objects in the bucket under the storage prefix.</p>"},{"location":"assets/storage_provider/#for-developers","title":"For developers","text":"<p>Developers may additionally need to be able to push new assets and or update existing assets, which requires them to be able to create and update certain objects.</p>"},{"location":"assets/storage_provider/#using-different-providers","title":"Using different providers","text":"<p>The flavor of the remote store that is used depends on optional dependencies used during pip install and on the <code>MODELKIT_STORAGE_PROVIDER</code> environment variable.</p> <p>The default <code>pip install modelkit</code> will only allow you to target a local directory.</p>"},{"location":"assets/storage_provider/#using-aws-s3-storage","title":"Using AWS S3 storage","text":"<p>Use <code>pip install modelkit[assets-s3]</code> and setup this environment variable <code>MODELKIT_STORAGE_PROVIDER=s3</code> to connect to S3 storage.</p> <p>We use boto3 under the hood.</p> <p>The authentication information here is passed to the <code>boto3.client</code> object:</p> Environment variable boto3.client argument <code>AWS_ACCESS_KEY_ID</code> <code>aws_access_key_id</code> <code>AWS_SECRET_ACCESS_KEY</code> <code>aws_secret_access_key</code> <code>AWS_SESSION_TOKEN</code> <code>aws_session_token</code> <code>AWS_DEFAULT_REGION</code> <code>region_name</code> <code>S3_ENDPOINT</code> <code>endpoint_url</code> <p>Typically, if you use AWS: having <code>AWS_DEFAULT_PROFILE</code>, <code>AWS_DEFAULT_REGION</code> and valid credentials in <code>~/.aws</code> is enough.</p> <p>S3 storage driver is compatible with KMS encrypted s3 volumes. Use <code>AWS_KMS_KEY_ID</code> environment variable to set your key and be able to upload files to such volume.</p>"},{"location":"assets/storage_provider/#gcs-storage","title":"GCS storage","text":"<p>Use <code>pip install modelkit[assets-gcs]</code> and setup this environment variable <code>MODELKIT_STORAGE_PROVIDER=gcs</code> to connect to GCS storage.</p> <p>We use google-cloud-storage.</p> Environment variable Default value Notes <code>GOOGLE_APPLICATION_CREDENTIALS</code> None path to the JSON file <p>By default, the GCS client use the credentials setup up on the machine.</p> <p>If <code>GOOGLE_APPLICATION_CREDENTIALS</code> is provided, it should point to a local JSON service account file, which we use to instantiate the client with <code>google.cloud.storage.Client.from_service_account_json</code></p>"},{"location":"assets/storage_provider/#using-azure-blob-storage","title":"Using Azure blob storage","text":"<p>Use <code>pip install modelkit[assets-az]</code> and setup this environment variable <code>MODELKIT_STORAGE_PROVIDER=az</code> to connect to Azure blob storage.</p> <p>We use azure-storage-blobl under the hood.</p> <p>The client is created by passing the authentication information to <code>BlobServiceClient.from_connection_string</code>:</p> Environment variable Note <code>AZURE_STORAGE_CONNECTION_STRING</code> azure connection string"},{"location":"assets/storage_provider/#local-mode","title":"<code>local</code> mode","text":"<p>Setup this environment variable <code>MODELKIT_STORAGE_PROVIDER=local</code> to treat a local folder as a remote source.</p> <p>Assets will be downloaded from this folder to the configured asset dir.</p> <p>If you would like to run on already downloaded assets please refer to \"Pre-downloaded mode\" section below.</p> Environment variable Notes <code>MODELKIT_STORAGE_BUCKET</code> path to the local folder <code>MODELKIT_STORAGE_PREFIX</code> sub directory where assets are fetched from"},{"location":"assets/storage_provider/#pre-downloaded-mode","title":"Pre-downloaded mode","text":"<p>Use <code>MODELKIT_STORAGE_PROVIDER=</code> or unset it to use only assets available in assets dir.</p> <p>This can be used for local development or for deploying assets in read-only artifacts.</p>"},{"location":"assets/storage_provider/#other-options","title":"Other options","text":"<p>If you would like us to support other means of remote storage, do feel free to request it by posting an issue!</p>"},{"location":"assets/store_organization/","title":"Remote store organization","text":""},{"location":"assets/store_organization/#remote-asset-storage-convention","title":"Remote asset storage convention","text":""},{"location":"assets/store_organization/#data-object","title":"Data object","text":"<p>Remote assets are stored in object stores, referenced as:</p> <pre><code>[provider]://[bucket]/[prefix]/[category]/[name]/[version]\n</code></pre> <p>In this \"path\":</p> <ul> <li><code>provider</code> is <code>s3</code>, <code>azfs</code> or <code>gcs</code> or <code>file</code> depending on the storage driver (value of <code>MODELKIT_STORAGE_PROVIDER</code>)</li> <li><code>bucket</code> is the remote container name (<code>MODELKIT_STORAGE_BUCKET</code>)</li> </ul> <p>The rest of the \"path\" is the remote object's name and consists of</p> <ul> <li><code>-prefix</code> is a prefix to all assets for a given <code>AssetsManager</code> (<code>MODELKIT_STORAGE_PREFIX</code>)</li> <li><code>name</code> describes the asset. The name may contain path separators <code>/</code> but each file remotely will be stored as a single object.</li> <li><code>version</code> describes the asset version in the form <code>X.Y</code></li> </ul> <p>Note</p> <p>If the asset is a directory, all sub files will be stored as  separate objects under this prefix.</p>"},{"location":"assets/store_organization/#meta-object","title":"Meta object","text":"<p>In addition to the data, the asset object reside alongside a <code>*.meta</code> object:</p> <pre><code>[provider]://[bucket]/[prefix]/[category]/[name]/[version].meta\n</code></pre> <p>The <code>meta</code> is a JSON file containing</p> <pre><code>{\n    \"push_date\": \"\", # ISO date of push\n    \"is_directory\": True or False, # whether the asset has mulitple objects\n    \"contents\": [] #\u00a0list of suffixes of contents when is_directory is True\n}\n</code></pre>"},{"location":"assets/store_organization/#version-object","title":"Version object","text":"<p>Assets have versions, following a <code>Major.Minor</code> version convention.</p> <p>We maintain a <code>versions</code> JSON file in order to keep track of the latest version.  This avoids having to list objects by prefix on objects store, which is typically a very time consuming query.</p> <p>It is stored at <pre><code>[provider]://[bucket]/[assetsmanager-prefix]/[category]/[name].versions\n</code></pre></p> <p>And contains <pre><code>{\n    \"versions\": [],\u00a0# list of ordered versions, latest first\n}\n</code></pre></p> <p>Note</p> <p>This is the only object on the object store that is not immutable. It is updated whenever an asset is updated.</p>"},{"location":"assets/versioning/","title":"Versioning","text":"<p>2 versioning systems are implemented by default in the <code>modelkit/assets/versioning</code> repository :</p> <ul> <li><code>major_minor</code> (used by default)</li> <li><code>simple_date</code></li> </ul> <p>We can implement a new versioning system by inheriting from the <code>AssetsVersioningSystem</code> class and override several methods which are necessary for the system to function.</p> <p>To use a specific system use the environment variable <code>MODELKIT_ASSETS_VERSIONING_SYSTEM</code></p>"},{"location":"assets/versioning/#assetsversioningsystem","title":"AssetsVersioningSystem","text":"<p>Some abstract functions have to be overriden :</p> <ul> <li> <p><code>get_initial_version</code> which returns the initial version of a new asset.</p> </li> <li> <p><code>check_version_valid</code> which checks if a given version is valid.</p> </li> <li> <p><code>sort_versions</code> which implements the sorting logic of versions (used for example to get the latest version).</p> </li> <li> <p><code>increment_version</code> which implements the version incrementation logic.</p> </li> <li> <p><code>get_update_cli_params</code> which specifies the  <code>update cli</code> display and the <code>increment_version</code> params received from parameters given to the <code>update cli</code>.</p> </li> </ul> <p>other methods can be overriden if needed.</p> <ul> <li> <p><code>is_version_complete</code> returns <code>True</code> by default but can be overriden for system which allows incomplete version to specify if a given is complete or not.</p> </li> <li> <p><code>get_latest_partial_version</code> returns the last version by default but can be overriden for system which allow incomplete version to filter versions corresponding to a given incomplete version.</p> </li> </ul>"},{"location":"assets/versioning/#simple-date","title":"Simple Date","text":"<p>Simple date is a very simple versioning system using current date in UTC ISO 8601 Z format as version :  <code>YYYY-MM-DDThh-mm-ssZ</code> (due to filename constraints the <code>:</code> of <code>hh:mm:ss</code> in the original notation are replaced with <code>-</code> ).</p> <p><code>get_initial_version</code> returns the current date in UTC iso format.</p> <p><code>increment_version</code> returns the current date in UTC iso format.</p> <p><code>sort_versions</code> iso format is compatible with a reversed alpha-numerical order.</p>"},{"location":"assets/versioning/#major-minor-default","title":"Major / Minor (default)","text":"<p>The major / minor system uses a <code>x.y</code> format where <code>x</code> is the <code>major</code> and <code>y</code> the <code>minor</code> version.</p> <p><code>get_initial_version</code> returns <code>0.0</code>.</p> <p><code>increment_version</code> increments the <code>minor</code> version (<code>y+=1</code>). A <code>bump_version</code> parameter allow us to increment a new <code>major</code> version  (<code>x+=1 ; y=0</code>).</p> <p><code>sort_versions</code> sorts according to a <code>(major, minor)</code> key, considering <code>major</code> and <code>minor</code> as integers.</p> <p>major/minor is compatible with incomplete versioning : version <code>x</code> corresponds to the latest <code>x.y</code> version :</p> <p><code>is_version_complete</code> returns <code>True</code> if <code>major</code> AND <code>minor</code> are specified and <code>False</code> if only the <code>major</code> is specified.</p> <p><code>get_latest_partial_version</code> returns the latest version OR the latest <code>x.y</code> version for a specified major <code>x</code>.</p>"},{"location":"contributions/release/","title":"Releasing modelkit","text":""},{"location":"contributions/release/#prepare-releasing-branch","title":"Prepare releasing branch","text":"<pre><code>git remote add central https://github.com/Cornerstone-OnDemand/modelkit.git\ngit fetch central main\ngit switch -C releasing central/main\n</code></pre>"},{"location":"contributions/release/#bump-version","title":"Bump version","text":"<p>We use <code>bump-my-version</code> to create the commit and tag required for the release <pre><code>bump-my-version bump patch\n</code></pre> or via its alias: <pre><code>bumpversion bump patch\n</code></pre></p>"},{"location":"contributions/release/#push-commit-and-tag-to-central","title":"Push commit and tag to central","text":"<pre><code>git push --follow-tags central releasing:main\n\n# Or more concisely if you have configured git with push.default = upstream\ngit push --follow-tags\n</code></pre>"},{"location":"contributions/release/#package-and-publish-new-artifact-to-pypi","title":"Package and publish new artifact to pypi","text":"<pre><code>pip wheel --no-deps .\npython -m twine upload modelkit-$(git describe --tags | cut -c 2-)-py3-none-any.whl\n</code></pre>"},{"location":"deployment/deployment/","title":"Automatic","text":"<p><code>modelkit</code> centralizes all of your models in a single object, which makes it easy to serve as a REST API via an HTTP server.</p> <p>Of course, you can do so using your favourite framework, ours is fastapi, so we have integrated several methods to make it easy to serve your models directly with it.</p>"},{"location":"deployment/deployment/#using-uvicorn","title":"Using uvicorn","text":"<p>A single CLI call will expose your models using <code>uvicorn</code>:</p> <pre><code>modelkit serve PACKAGE --required-models REQUIRED_MODELS --host HOST --port PORT\n</code></pre> <p>This will create a server with a single worker with all the models listed in <code>REQUIRED_MODELS</code>, as configured in the <code>PACKAGE</code>.</p> <p>Each <code>Model</code> configured as <code>model_name</code> will have two POST endpoints:</p> <ul> <li><code>/predict/model_name</code> which accepts single items</li> <li><code>/predict/batch/model_name</code> which accepts lists of items</li> </ul> <p>Endpoint payloads and specs are parametrized by the <code>pydantic</code> payloads that it uses to validate its inputs and outputs. </p> <p>Head over to <code>/docs</code> to check out the swagger and try your models out!</p>"},{"location":"deployment/deployment/#using-gunicorn-with-multiple-workers","title":"Using gunicorn with multiple workers","text":"<p>For more performance, <code>modelkit</code> allows you to use <code>gunicorn</code> with multiple workers that share the same <code>ModelLibrary</code>: <code>modelkit.api.create_modelkit_app</code>.</p> <p>It can take all its arguments through environment variables, so you can run a server quickly, for example:</p> <pre><code>export MODELKIT_DEFAULT_PACKAGE=my_package\nexport PYTHONPATH=path/to/the/package\ngunicorn \\\n    --workers 4 \\\n    --preload \\\n    --worker-class=uvicorn.workers.UvicornWorker \\\n    'modelkit.api:create_modelkit_app()'\n</code></pre> <p>Note</p> <p>Since <code>ModelLibrary</code> is shared between the workers, therefore adding workers will not increase the memory footprint.</p>"},{"location":"deployment/deployment/#automatic-endpoints-router-in-fastapi","title":"Automatic endpoints router in fastAPI","text":"<p>If you are interested in adding all <code>Model</code> endpoints in an existing <code>fastapi</code> application, you can also use the <code>modelkit.api.ModelkitAutoAPIRouter</code>:</p> <pre><code>app = fastapi.FastAPI()\nrouter = ModelkitAutoAPIRouter(\n    required_models=required_models,\n    models=models)\napp.include_router(router)\n</code></pre> <p>Which will include one endpoint for each model in <code>required_models</code>, pulled form the <code>models</code> package.</p> <p>To override the route paths for individual models, use <code>route_paths</code>:</p> <pre><code>router = ModelkitAutoAPIRouter(\n    required_models=required_models,\n    models=models,\n    route_paths={\n        \"some_model\": \"/a/new/path\"\n    }\n)\n</code></pre>"},{"location":"deployment/integrate_fastapi/","title":"Integrate in existing app","text":"<p>Integrating <code>modelkit</code> models in an existing app is extremely simple, you simply have to add the <code>ModelLibrary</code> object to the main module.</p>"},{"location":"deployment/integrate_fastapi/#instantiate-a-modellibrary","title":"Instantiate a ModelLibrary","text":"<p>In the main Python script where you define your fastapi application, attach the model library to the application state:</p> <pre><code>import fastapi\nimport modelkit\n\napp = fastapi.FastAPI()\n# instantiate the library\nlib = modelkit.ModelLibrary(\n    ...\n)\n# add the library to the app state\napp.state.lib = lib\n\n\n# Don't forget to close the connections when the application stops!\n@app.on_event(\"shutdown\")\nasync def shutdown_event():\n    await app.state.lib.aclose()\n</code></pre> Multiple workers <p>This method of integrating the <code>ModelLibrary</code> ensures that all models are created before different workers are instantiated (e.g. using <code>gunicorn --preload</code>), which is convenient since all will share the same model objects and not increase memory.</p>"},{"location":"deployment/integrate_fastapi/#use-the-models","title":"Use the models","text":"<p>Finally, anywhere else where you add endpoints to the application, you can retrieve the <code>ModelLibrary</code> from the <code>request</code> object.</p> <p>Since getting the <code>Model</code> object is just a dictionary lookup, retrieving it is instantaneous.</p> <pre><code>@app.post(...)\ndef some_path_endpoint(request: fastapi.Request, item):\n    # Get the model object\n    m = request.app.state.lib.get(\"model_name\")\n    # Use to make predictions as usual\n    m.predict(...)\n    ...\n    return ...\n</code></pre> Async support <p>This is the context in which <code>modelkit</code>'s async support shines, be sure to use your <code>AsyncModel</code>s here:</p> <pre><code>@app.post(...)\nasync def some_path_endpoint(request: fastapi.Request, item):\n    m = request.app.state.lib.get(\"model_name\")\n    result = await m.predict(...)\n    ...\n    return ...\n</code></pre>"},{"location":"examples/tf_hub/","title":"Loading Model from TF Hub","text":"<p>In this tutorial we will learn how to load a pre-trained Tensorflow saved model</p> <p>We will use the Universal Sentence Encoder from tfhub as an example</p>"},{"location":"examples/tf_hub/#download-and-extract","title":"Download and extract","text":"<p>Fisrt, download the file <code>universal-sentence-encoder-multilingual_3.tar.gz</code> and extract it in a <code>asset_name/1</code> directory</p> <p>In this tutorial we will use <code>/tmp/use/1</code></p> <p><code>tar -xzvf universal-sentence-encoder-multilingual_3.tar.gz --directory /tmp/use/1</code></p> <p>this will create the following tree:</p> <pre><code>use\n\u2514\u2500\u2500 1\n    \u251c\u2500\u2500 assets\n    \u251c\u2500\u2500 saved_model.pb\n    \u2514\u2500\u2500 variables\n        \u251c\u2500\u2500 variables.data-00000-of-00001\n        \u2514\u2500\u2500 variables.index\n</code></pre>"},{"location":"examples/tf_hub/#check-the-model-configuration","title":"Check the model configuration","text":"<p>In order to use <code>modelkit</code> to use the model we have to check the model configuation in order to exctract outputs informations (key name, layer name, shape and type) and the inputs key name:</p> <pre><code>import tensorflow as tf\nimport tensorflow_text  # module need by use model\nmodel = tf.saved_model.load(\"/tmp/use/1/\")\nprint(model.signatures[\"serving_default\"].output_dtypes)\nprint(model.signatures[\"serving_default\"].output_shapes)\nprint(model.signatures[\"serving_default\"].outputs)\nprint(model.signatures[\"serving_default\"].inputs[0])\n\n#\u00a0Output:\n#\u00a0{'outputs': tf.float32}\n#\u00a0{'outputs': TensorShape([None, 512])}\n#\u00a0[&lt;tf.Tensor 'Identity:0' shape=(None, 512) dtype=float32&gt;]\n#\u00a0&lt;tf.Tensor 'inputs:0' shape=(None,) dtype=string&gt;\n</code></pre>"},{"location":"examples/tf_hub/#quick-load-with-modelkit","title":"Quick load with <code>modelkit</code>","text":"<p>We can now load the model by creating a <code>TensorflowModel</code> class and configuring it with information we just got from the model.</p> <p>Note that we have to declare a \"virtual\" asset <code>\"asset\": \"use\"</code> to directly set an asset path <code>/tmp/use</code> (without the 1 directory)</p> <pre><code>import numpy as np\nimport tensorflow_text\n\nimport modelkit\nfrom modelkit.core.models.tensorflow_model import TensorflowModel\n\nclass USEModel(TensorflowModel):\n    CONFIGURATIONS = {\n        \"use\": {\n            \"asset\": \"use\",\n            \"model_settings\": {\n                \"output_tensor_mapping\": {\"outputs\": \"Identity:0\"},\n                \"output_shapes\": {\"outputs\": (512,)},\n                \"output_dtypes\": {\"outputs\": np.float32},\n                \"asset_path\": \"/tmp/use\",\n            },\n        }\n    }\n</code></pre> <p>and then we can test it using <code>load_model</code></p> <pre><code>model = modelkit.load_model(\"use\", models=USEModel)\nmodel.predict({\"inputs\": \"Hello world\"})\n#\u00a0note that the \"inputs\" keyword is extracted from the previous model configuration\n</code></pre> <p>That's all !</p> <p>We can start testing/using our model</p> <pre><code>from sklearn.metrics.pairwise import cosine_distances\n\nsentence_1 = model.predict({\"inputs\": \"My dog is quite calm today\"})[\"outputs\"]\nsentence_2 = model.predict({\"inputs\": \"Mon chien est assez calme aujourd'hui\"})[\"outputs\"]\nsentence_3 = model.predict({\"inputs\": \"It rains on my house\"})[\"outputs\"]\nsentence_4 = model.predict({\"inputs\": \"Il pleut sur ma maison\"})[\"outputs\"]\n\nprint(cosine_similarity([sentence_1, sentence_2, sentence_3, sentence_4]))\n# output :\n#\u00a0[[1.         0.93083745 0.3172922  0.3379839 ]\n#\u00a0 [0.93083745 0.99999994 0.3522399  0.39009082]\n#\u00a0 [0.3172922  0.3522399  0.9999999  0.8444551 ]\n#\u00a0 [0.3379839  0.39009082 0.8444551  0.9999999 ]]\n\n#\u00a0 - sentence_1 close to sentence_2 (0.93)\n#\u00a0 - sentence_3 close to sentence_4 (0.84)\n#  - other distances &lt; 0.5\n\n#\u00a0=&gt; seems to work :-)\n</code></pre>"},{"location":"examples/tf_hub/#create-an-asset","title":"Create an asset","text":"<p>Once we have tested our model, we may want push and use it as an versioned asset</p> <p><code>modelkit assets new /tmp/use my_assets/use</code></p> <p>and then we can remove <code>asset_path</code> and add our <code>asset</code> name to our <code>TensorflowModel</code></p> <pre><code>import numpy as np\nimport tensorflow_text\n\nimport modelkit\nfrom modelkit.core.models.tensorflow_model import TensorflowModel\n\nclass USEModel(TensorflowModel):\n    CONFIGURATIONS = {\n        \"use\": {\n            \"asset\": \"my_assets/use:0.0\",\n            \"model_settings\": {\n                \"output_tensor_mapping\": {\"outputs\": \"Identity:0\"},\n                \"output_shapes\": {\"outputs\": (512,)},\n                \"output_dtypes\": {\"outputs\": np.float32},\n            },\n        }\n    }\n</code></pre> <p>and then we may use a <code>ModelLibrary</code> to load it</p> <pre><code>model_library = modelkit.ModelLibrary(\n    required_models=[\"use\"],\n    models=USEModel,\n)\nmodel = model_library.get(\"use\")\nmodel.predict({\"inputs\": \"Hello world\"})\n</code></pre>"},{"location":"examples/tf_hub/#using-tensorflow-serving","title":"Using tensorflow serving","text":"<p>Our model is directly compatible with our <code>tensorflow-serving</code> loading scripts</p> <p>Let's say we have saved our model in <code>modelkit/use.py</code> file, generate the configuation:</p> <p><code>modelkit tf-serving local-docker modelkit.use -r \"use\"</code></p> <p>it will create a config file <code>${MODELKIT_ASSETS_DIR}/config.config</code></p> <p>then we can start our <code>tf-serving</code> running</p> <p><code>docker run --name local-tf-serving -d -p 8500:8500 -p 8501:8501 -v ${MODELKIT_ASSETS_DIR}:/config -t tensorflow/serving --model_config_file=/config/config.config --rest_api_port=8501 --port=8500</code></p> <p>then we can try use use our model with <code>MODELKIT_TF_SERVING_ENABLE=1</code>, we should see the log line when loading the model</p> <p><code>[info     ] Connecting to tensorflow serving mode=rest model_name=use port=8501 tf_serving_host=localhost</code></p>"},{"location":"examples/nlp_sentiment/advanced_models/","title":"Advanced models","text":"<p>You have now seen most development features and how to implement everything from the Tokenizer to the Classifier.</p> <p>Let's see how we can put it all together, while reviewing all Modelkit's features we have seen so far and introducing a powerful new feature: model dependencies.</p>"},{"location":"examples/nlp_sentiment/advanced_models/#composing-models","title":"Composing models","text":"<p><code>modelkit</code>'s allows you to add models as dependencies of other models, and use them in the <code>_predict</code> methods.</p> <p>For example, it is desirable for our classifier to take string reviews as input, and output a class label (\"good\" or \"bad\"). This can be achieved using model dependencies.</p> <p>We need to specify the <code>model_dependencies</code> key in the <code>CONFIGURATIONS</code> map to add the <code>Tokenizer</code> and the <code>Vectorizer</code> as dependencies:</p> <pre><code>import modelkit\n\nclass Classifier(modelkit.Model[str, str]):\n    CONFIGURATIONS = {\n        \"imdb_classifier\": {\n            \"asset\": \"imdb_model.h5\",\n            \"model_dependencies\": {\"imdb_tokenizer\", \"imdb_vectorizer\"}\n        },\n    }\n</code></pre> <p>The <code>modelkit.ModelLibrary</code> will take care of loading the model dependencies  before your model is available. They are then made available in the <code>model_dependencies</code> attribute, and can readily be used in the <code>_predict</code> method.</p> <p>For example:</p> <pre><code>import modelkit\n\nclass Classifier(modelkit.Model[str, str]):\n    CONFIGURATIONS = {\n        \"imdb_classifier\": {\n            \"asset\": \"imdb_model.h5\",\n            \"model_dependencies\": {\"imdb_tokenizer\", \"imdb_vectorizer\"}\n        },\n    }\n    ...\n    def _predict_batch(self, reviews):\n        # this looks like the previous end-to-end example from the previous section\n        tokenized_reviews = self.model_dependencies[\"imdb_tokenizer\"].predict_batch(reviews)\n        vectorized_reviews = self.model_dependencies[\"imdb_vectorizer\"].predict_batch(tokenized_reviews, length=64)\n        predictions_scores = self.model.predict(vectorized_reviews)\n        predictions_classes = [\"good\" if score &gt;= 0.5 else \"bad\" for score in predictions_scores]\n        return predictions_classes\n</code></pre> <p>This end-to-end classifier is much easier to use, and still loadable easily: let us use a <code>ModelLibrary</code>:</p> <pre><code># define the model library with all necessary models\nmodel_library = modelkit.ModelLibrary(models=[Tokenizer, Vectorizer, Classifier])\n\n# this will load all models\nclassifier = model_library.get(\"imdb_classifier\")\nclassifier.predict(\"This movie is freaking awesome, I love the main character\")\n# good \n</code></pre>"},{"location":"examples/nlp_sentiment/advanced_models/#adding-complex-output","title":"Adding complex output","text":"<p>We may want to output the score as well as a class when using the model. </p> <p>Although we could return a dictionary, this can just as easily be achieved by outputing a Pydantic model, which offers more advanced validation features (and is used under the hood in <code>modelkit</code>), and can readily be used in fastapi endpoints.</p> <p>This is really good practice, as it makes your code more understandable (even more so as your number of models grow).</p> <p>Let us modify our code to specify a <code>pydantic.BaseModel</code> as the output of our <code>Model</code>, which contains a label and the score.</p> <pre><code>import modelkit\nimport numpy as np\nimport pydantic\nimport tensorflow as tf\n\nclass MovieSentimentItem(pydantic.BaseModel):\n    label: str\n    score: float\n\nclass Classifier(modelkit.Model[str, MovieSentimentItem]):\n    CONFIGURATIONS = {\n        \"imdb_classifier\": {\n            \"asset\": \"imdb_model.h5\",\n            \"model_dependencies\": {\"imdb_tokenizer\", \"imdb_vectorizer\"},\n        },\n    }\n    TEST_CASES = [\n        {\n            \"item\": {\"text\": \"i love this film, it's the best I've ever seen\"},\n            \"result\": {\"score\": 0.8441019058227539, \"label\": \"good\"},\n        },\n        {\n            \"item\": {\"text\": \"this movie sucks, it's the worst I have ever seen\"},\n            \"result\": {\"score\": 0.1625385582447052, \"label\": \"bad\"},\n        },\n    ]\n\n    def _load(self):\n        self.model = tf.keras.models.load_model(self.asset_path)\n        self.tokenizer = self.model_dependencies[\"imdb_tokenizer\"]\n        self.vectorizer = self.model_dependencies[\"imdb_vectorizer\"]\n\n    def _predict_batch(self, reviews):\n        texts = [reviews.text for review in reviews]\n        tokenized_reviews = self.tokenizer.predict_batch(texts)\n        vectorized_reviews = self.vectorizer.predict_batch(tokenized_reviews, length=64)\n        predictions_scores = self.model.predict(vectorized_reviews)\n        return [\n            {\"score\": score, \"label\": \"good\" if score &gt;= 0.5 else \"bad\"}\n            for score in predictions_scores\n        ]\n</code></pre> <p>We also added some <code>TEST_CASES</code> to make sure that our model still behaves correctly.</p> <p>Note</p> <p>Although we return a dictionary, it will be turned into a <code>MovieSentimentItem</code>, and validated by Pydantic.</p> <p>We can now test the <code>Model</code>:</p> <pre><code>model_library = modelkit.ModelLibrary(models=[Tokenizer, Vectorizer, Classifier])\nclassifier = model_library.get(\"imdb_classifier\")\nclassifier.test()\nprediction = classifier.predict({\"text\": \"I love the main character\"})\nprint(prediction)\n# MovieSentimentItem(label='good', score=0.6801363825798035)\nprint(prediction.label) \n# good\nprint(prediction.score) \n# 0.6801363825798035\n</code></pre>"},{"location":"examples/nlp_sentiment/advanced_models/#multiple-model-configurations","title":"Multiple Model configurations","text":"<p>To conclude this tutorial, let us briefly see how to define multiple configurations, and why one would want to do that.</p> <p>Assume that our <code>Classifier</code> model goes to production and most users are happy with it, but some are not. You start from scratch: redefine a tokenizer, vectorizer, train a new classifier with a different architecture and more data, and save it preciously.</p> <p>Since you have always been the original guy in the room, all the new models now have a \"_SOTA\" suffix. You managed to keep the same inputs, outputs, pipeline architecture and made your process reproductible.</p> <p>\"That should do it !\", you yell across the open space.</p> <p>However, you probably do not want to surprise your clients with a new model without informing them before.Some might want to stick with the old one, while the unhappy ones would want to change ASAP.</p> <p>Modelkit has got your back, and allows you to define multiple configurations while keeping the exact same code.</p> <p>Here is how you go about doing this:</p> <pre><code>import modelkit\n\nclass Classifier(modelkit.Model[MovieReviewItem, MovieSentimentItem]):\n    CONFIGURATIONS = {\n        \"classifier\": {\n            \"asset\": \"imdb_model.h5\",\n            \"model_dependencies\": {\n                \"tokenizer\": \"imdb_tokenizer\",\n                \"vectorizer\": \"imdb_vectorizer\",\n            },\n            \"test_cases\": [\n                {\n                    \"item\": {\"text\": \"i love this film, it's the best i've ever seen\"},\n                    \"result\": {\"score\": 0.8441019058227539, \"label\": \"good\"},\n                },\n                {\n                    \"item\": {\n                        \"text\": \"this movie sucks, it's the worst I have ever seen\"\n                    },\n                    \"result\": {\"score\": 0.1625385582447052, \"label\": \"bad\"},\n                },\n            ],\n        },\n        \"classifier_SOTA\": {\n            \"asset\": \"imdb_model_SOTA.h5\",\n            \"model_dependencies\": {\n                \"tokenizer\": \"imdb_tokenizer_SOTA\",\n                \"vectorizer\": \"imdb_vectorizer_SOTA\",\n            },\n            \"test_cases\": [\n                {\n                    \"item\": {\"text\": \"i love this film, it's the best i've ever seen\"},\n                    \"result\": {\"score\": 1.0, \"label\": \"good\"},\n                },\n                {\n                    \"item\": {\n                        \"text\": \"this movie sucks, it's the worst I have ever seen\"\n                    },\n                    \"result\": {\"score\": 0.0, \"label\": \"bad\"},\n                },\n            ],\n        },\n    }\n    ...\n</code></pre> <p>In order to use the same code within <code>_predict</code>, we have renamed the dependencies, by using a dictionary instead of a set in <code>model_dependencies</code>.</p> <p>The <code>model_dependencies</code> attribute of <code>classifier</code> and <code>classifier_SOTA</code> will have the same <code>tokenizer</code> and <code>vectorizer</code> keys, but pointing to different <code>Model</code>s. Also, the <code>tests_cases</code> are now part of each individual configuration so that to test each one independently.</p> <p>Now both of your models are available through the same library:</p> <pre><code>model_library = modelkit.ModelLibrary(models=[Tokenizer, Vectorizer, Classifier])\nclassifier_deprecated = model_library.get(\"classifier\")\nclassifier_SOTA = model_library.get(\"classifier_SOTA\")\n</code></pre> <p>Additionally, you can decide to filter out which one you are interested in (and avoid it being loaded in memory if it is unused), by specifying the <code>required_models</code> keyword argument:</p> <pre><code>model_library = modelkit.ModelLibrary(\n    required_models=[\"classifier_SOTA\"]\n    models=[Tokenizer, Vectorizer, Classifier]\n)\n</code></pre> <p>As you can see, the <code>CONFIGURATIONS</code> map and the dependency renaming helped make this task easier than we may have thought in the first instance.</p>"},{"location":"examples/nlp_sentiment/classifier/","title":"Classifier","text":"<p>In this section, we will implement a sentiment classifier with Keras, using the components we have been developping so far.</p>"},{"location":"examples/nlp_sentiment/classifier/#data-preparation","title":"Data preparation","text":"<p>We will reuse the <code>read_dataset</code> function, in addition to a few other helper functions which will continue taking advantage of generators to avoid loading up the entire dataset into memory:</p> <ul> <li><code>alternate(f, g)</code>: to yield items alternatively between the f and g generators</li> <li><code>alternate_labels()</code>: to yield 1 and 0, alternatively, in sync with the previous <code>alternate</code> function</li> </ul> <pre><code>import glob\nimport os\n\nfrom typing import Generator, Any\n\ndef read_dataset(path: str) -&gt; Generator[str, None, None]:\n    for review in glob.glob(os.path.join(path, \"*.txt\")):\n        with open(review, 'r', encoding='utf-8') as f:\n            yield f.read()\n\ndef alternate(\n    f: Generator[Any, Any, Any], g: Generator[Any, Any, Any]\n) -&gt; Generator[Any, None, None]:\n    while True:\n        try:\n            yield next(f)\n            yield next(g)\n        except StopIteration:\n            break\n\ndef alternate_labels() -&gt; Generator[int, None, None]:\n    while True:\n        yield 1\n        yield 0\n</code></pre> <p>Let's plug these pipes together so that to efficiently read and process the IMDB reviews dataset for our next Keras classifier, leveraging the Tokenizer and Vectorizer we implemented in the first two sections:</p> <pre><code>def process(path, tokenizer, vectorizer, length, batch_size):\n    # read the positive sentiment reviews dataset\n    positive_dataset = read_dataset(os.path.join(path, \"pos\"))\n    # read the negative sentiment reviews dataset\n    negative_dataset = read_dataset(os.path.join(path, \"neg\"))\n    # alternate between positives and negatives examples\n    dataset = alternate(positive_dataset, negative_dataset)\n    # generate labels in sync with the previous generator: 1 for positive examples, 0 for negative ones\n    labels = alternate_labels()\n\n    # tokenize the reviews using our Tokenizer Model\n    tokenized_dataset = tokenizer.predict_gen(dataset, batch_size=batch_size)\n    # vectorize the reviews using our Vectorizer Model\n    vectorized_dataset = vectorizer.predict_gen(\n        tokenized_dataset, length=length, batch_size=batch_size\n    )\n    # yield (review, label) tuples for Keras\n    yield from zip(vectorized_dataset, labels)\n</code></pre> <p>Let's try it out on the first examples:</p> <pre><code>i = 0\nfor review, label in process(\n    os.path.join(\"aclImdb\", \"train\"),\n    modelkit.load_model(\"imdb_tokenizer\", models=Tokenizer),\n    modelkit.load_model(\"imdb_vectorizer\", models=Vectorizer),\n    length=64,\n    batch_size=64,\n):\n    print(review, label)\n    i += 1\n    if i &gt;= 10:\n        break\n</code></pre>"},{"location":"examples/nlp_sentiment/classifier/#model-library","title":"Model library","text":"<p>So far, we have instantiated <code>Tokenizer</code> and <code>Vectorizer</code> classes as standard objects. Modelkit provides a simpler and more powerful way to instantiate <code>Models</code>, the library: <code>modelkit.ModelLibrary</code>.</p> <p>The purpose of the <code>ModelLibrary</code> is to have a single way to load any of the models that are defined, any way you decide to keep them organized.</p> <p>With a library <code>ModelLibrary</code> you can</p> <ul> <li>fetch models by their configuration key using <code>ModelLibrary.get</code> while ensuring that models are only loaded once</li> <li>use prediction caching: Prediction Caching</li> <li>use lazy loading for models: Lazy Loading</li> <li>override model parameters: Settings</li> </ul> <p>Although we will not cover all of these features here, let's see how we can take advantage of the <code>ModelLibrary</code> with our previous work.</p> <p>Let us define a model library, it can take the clases of models as input:</p> <pre><code>import modelkit\n\n... #\u00a0define Vectorizer and Tokenizer classes\n\nmodel_library = modelkit.ModelLibrary(models=[Vectorizer, Tokenizer])\ntokenizer = model_library.get(\"imdb_tokenizer\")\nvectorizer = model_library.get(\"imdb_vectorizer\")\n</code></pre> <p>Or, alternatively use the modules they are present in:</p> <pre><code>import module_with_models\n\nmodel_library = modelkit.ModelLibrary(models=module_with_models)\n</code></pre> <p>This method is the preferred method, because it encourages you to adopt a package-like organisation of your <code>Models</code> (see Organizing Models)</p>"},{"location":"examples/nlp_sentiment/classifier/#using-models-to-create-a-tfdataset","title":"Using models to create a TF.Dataset","text":"<p>Now that we know how to reach out models, let us use them to create a <code>TF Dataset</code> from our data processing generators:</p> <pre><code>import os\nimport tensorflow as tf\n\nBATCH_SIZE = 64\nLENGTH = 64\n\ntraining_set = (\n    tf.data.Dataset.from_generator(\n        lambda: process(\n            os.path.join(\"aclImdb\", \"train\"),\n            tokenizer,\n            vectorizer,\n            length=LENGTH,\n            batch_size=BATCH_SIZE,\n        ),\n        output_types=(tf.int16, tf.int16),\n    )\n    .batch(BATCH_SIZE)\n    .repeat()\n    .prefetch(1)\n)\nvalidation_set = (\n    tf.data.Dataset.from_generator(\n        lambda: process(\n            os.path.join(\"aclImdb\", \"test\"),\n            tokenizer,\n            vectorizer,\n            length=LENGTH,\n            batch_size=BATCH_SIZE,\n        ),\n        output_types=(tf.int16, tf.int16),\n    )\n    .batch(BATCH_SIZE)\n    .repeat()\n    .prefetch(1)\n)\n</code></pre>"},{"location":"examples/nlp_sentiment/classifier/#training-a-keras-model","title":"Training a Keras model","text":"<p>Let's train a basic Keras classifier to predict whether an IMDB review is positive or negative, and save it to disk.</p> <pre><code>import tensorflow as tf\n\nmodel = tf.keras.Sequential(\n    [\n        tf.keras.layers.Embedding(\n            input_dim=len(vectorizer.vocabulary) + 2, output_dim=64, input_length=LENGTH\n        ),\n        tf.keras.layers.Lambda(lambda x: tf.reduce_sum(x, axis=1)),\n        tf.keras.layers.Dense(1, activation=\"sigmoid\"),\n    ]\n)\nmodel.compile(\n    tf.keras.optimizers.Adam(0.001),\n    loss=tf.keras.losses.BinaryCrossentropy(),\n    metrics=[tf.keras.metrics.binary_accuracy],\n)\nmodel.build()\nmodel.fit(\n    training_set,\n    validation_data=validation_set,\n    epochs=10,\n    steps_per_epoch=100,\n    validation_steps=10,\n)\nmodel.save(\n    \"imdb_model.h5\", include_optimizer=False, save_format=\"h5\", save_traces=False\n)\n</code></pre>"},{"location":"examples/nlp_sentiment/classifier/#classifier-model","title":"Classifier Model","text":"<p>Voil\u00e0 ! As we already did for the Vectorizer, we will embed the just-saved <code>imdb_model.h5</code> in a basic Modelkit <code>Model</code> which we will further upgrade in the next section.</p> <pre><code>import modelkit\nimport tensorflow as tf\n\nfrom typing import List\n\n\nclass Classifier(modelkit.Model[List[int], float]):\n    CONFIGURATIONS = {\"imdb_classifier\": {\"asset\": \"imdb_model.h5\"}}\n\n    def _load(self):\n        self.model = tf.keras.models.load_model(self.asset_path)\n\n    def _predict_batch(self, vectorized_reviews):\n        return self.model.predict(vectorized_reviews)\n</code></pre> <p>Much like we did for the <code>Vectorizer</code>, the <code>Classifier</code> model has a <code>imdb_classifier</code> configuration with an asset pointing to the <code>imdb_model.h5</code>.</p> <p>We also benefit from Keras' <code>predict</code> ability to batch predictions in our <code>_predict_batch</code> method.</p>"},{"location":"examples/nlp_sentiment/classifier/#end-to-end-prediction","title":"End-to-end prediction","text":"<p>To summarize, here is how we would want to chain our <code>Tokenizer</code>, <code>Vectorizer</code> and <code>Classifier</code>:</p> <pre><code>import modelkit\n\nlibrary = modelkit.ModelLibrary(models=[Tokenizer, Vectorizer, Classifier])\ntokenizer = library.get(\"imdb_tokenizer\")\nvectorizer = library.get(\"imdb_vectorizer\")\nclassifier = library.get(\"imdb_classifier\")\n\nreview = \"I freaking love this movie, the main character is so cool !\"\n\ntokenized_review = tokenizer(review)  # or tokenizer.predict\nvectorized_review = vectorizer(tokenized_review)  # or vectorizer.predict\nprediction = classifier(vectorized_review)  # or classifier.predict\n</code></pre> <p>In the next (and final) section, we will see how <code>modelkit</code> can be used to perform this operation in a single <code>Model</code>.</p>"},{"location":"examples/nlp_sentiment/intro/","title":"Intro","text":"<p>In this tutorial, you will learn how to leverage Modelkit as a support for a common NLP task: Sentiment Analysis.</p> <p>You will see how Modelkit can be useful as you progress along the following steps:</p> <ol> <li>Implementing the Tokenizer leveraging spaCy</li> <li>Implementing the Vectorizer leveraging Scikit-Learn</li> <li>Building a simple Classifier leveraging Keras to predict whether a review is negative or positive</li> </ol> <p>Last but not least, you will get to see how a breaze it is to speed up your go-to-production process with Modelkit. </p>"},{"location":"examples/nlp_sentiment/tldr/","title":"TL;DR","text":"<p>Here is the entire tutorial implementation, covering most Modelkit's features to get you started.</p> <pre><code>from typing import List, Optional\n\nimport modelkit\nimport numpy as np\nimport pydantic\nimport spacy\nimport tensorflow as tf\n\n\nclass Tokenizer(modelkit.Model[str, List[str]]):\n    CONFIGURATIONS = {\"imdb_tokenizer\": {}}\n    TEST_CASES = [\n        {\"item\": \"\", \"result\": []},\n        {\"item\": \"NLP 101\", \"result\": [\"nlp\"]},\n        {\n            \"item\": \"I'm loving the spaCy 101 course !!!\u00f9*`^@\ud83d\ude00\",\n            \"result\": [\"loving\", \"spacy\", \"course\"],\n        },\n        {\n            \"item\": \"&lt;br/&gt;prepare things for IMDB&lt;br/&gt;\",\n            \"result\": [\"prepare\", \"things\", \"imdb\"],\n        },\n        {\n            \"item\": \"&lt;br/&gt;a b c data&lt;br/&gt;      e scientist\",\n            \"result\": [\"data\", \"scientist\"],\n        },\n    ]\n\n    def _load(self):\n        self.nlp = spacy.load(\n            \"en_core_web_sm\",\n            disable=[\n                \"parser\",\n                \"ner\",\n                \"tagger\",\n                \"lemmatizer\",\n                \"tok2vec\",\n                \"attribute_ruler\",\n            ],\n        )\n\n    def _predict_batch(self, texts):\n        texts = [\n            \" \".join(text.replace(\"&lt;br\", \"\").replace(\"/&gt;\", \"\").split())\n            for text in texts\n        ]\n        return [\n            [\n                t.lower_\n                for t in text\n                if t.is_ascii\n                and len(t) &gt; 1\n                and not (t.is_punct or t.is_stop or t.is_digit)\n            ]\n            for text in self.nlp.pipe(texts, batch_size=len(texts))\n        ]\n\n\nclass Vectorizer(modelkit.Model[List[str], List[int]]):\n    CONFIGURATIONS = {\"imdb_vectorizer\": {\"asset\": \"vocabulary.txt\"}}\n    TEST_CASES = [\n        {\"item\": [], \"result\": []},\n        {\"item\": [], \"keyword_args\": {\"length\": 10}, \"result\": [0] * 10},\n        {\"item\": [\"movie\"], \"result\": [888]},\n        {\"item\": [\"unknown_token\"], \"result\": []},\n        {\n            \"item\": [\"unknown_token\"],\n            \"keyword_args\": {\"drop_oov\": False},\n            \"result\": [1],\n        },\n        {\"item\": [\"movie\", \"unknown_token\", \"scenes\"], \"result\": [888, 1156]},\n        {\n            \"item\": [\"movie\", \"unknown_token\", \"scenes\"],\n            \"keyword_args\": {\"drop_oov\": False},\n            \"result\": [888, 1, 1156],\n        },\n        {\n            \"item\": [\"movie\", \"unknown_token\", \"scenes\"],\n            \"keyword_args\": {\"length\": 10},\n            \"result\": [888, 1156, 0, 0, 0, 0, 0, 0, 0, 0],\n        },\n        {\n            \"item\": [\"movie\", \"unknown_token\", \"scenes\"],\n            \"keyword_args\": {\"length\": 10, \"drop_oov\": False},\n            \"result\": [888, 1, 1156, 0, 0, 0, 0, 0, 0, 0],\n        },\n    ]\n\n    def _load(self):\n        self.vocabulary = {}\n        with open(self.asset_path, \"r\", encoding=\"utf-8\") as f:\n            for i, k in enumerate(f):\n                self.vocabulary[k.strip()] = i + 2\n        self._vectorizer = np.vectorize(lambda x: self.vocabulary.get(x, 1))\n\n    def _predict(self, tokens, length=None, drop_oov=True):\n        vectorized = (\n            np.array(self._vectorizer(tokens), dtype=np.int)\n            if tokens\n            else np.array([], dtype=int)\n        )\n        if drop_oov and len(vectorized):\n            vectorized = np.delete(vectorized, vectorized == 1)\n        if not length:\n            return vectorized.tolist()\n        result = np.zeros(length)\n        vectorized = vectorized[:length]\n        result[: len(vectorized)] = vectorized\n        return result.tolist()\n\n\nclass MovieReviewItem(pydantic.BaseModel):\n    text: str\n    rating: Optional[float] = None  # could be useful in the future ? but not mandatory\n\n\nclass MovieSentimentItem(pydantic.BaseModel):\n    label: str\n    score: float\n\n\nclass Classifier(modelkit.Model[MovieReviewItem, MovieSentimentItem]):\n    CONFIGURATIONS = {\n        \"imdb_classifier\": {\n            \"asset\": \"imdb_model.h5\",\n            \"model_dependencies\": {\n                \"tokenizer\": \"imdb_tokenizer\",\n                \"vectorizer\": \"imdb_vectorizer\",\n            },\n        },\n    }\n    TEST_CASES = [\n        {\n            \"item\": {\"text\": \"i love this film, it's the best I've ever seen\"},\n            \"result\": {\"score\": 0.8441019058227539, \"label\": \"good\"},\n        },\n        {\n            \"item\": {\"text\": \"this movie sucks, it's the worst I have ever seen\"},\n            \"result\": {\"score\": 0.1625385582447052, \"label\": \"bad\"},\n        },\n    ]\n\n    def _load(self):\n        self.model = tf.keras.models.load_model(self.asset_path)\n        self.tokenizer = self.model_dependencies[\"tokenizer\"]\n        self.vectorizer = self.model_dependencies[\"vectorizer\"]\n\n    def _predict_batch(self, reviews):\n        texts = [review.text for review in reviews]\n        tokenized_reviews = self.tokenizer.predict_batch(texts)\n        vectorized_reviews = self.vectorizer.predict_batch(tokenized_reviews, length=64)\n        predictions_scores = self.model.predict(vectorized_reviews)\n        predictions = [\n            {\"score\": score, \"label\": \"good\" if score &gt;= 0.5 else \"bad\"}\n            for score in predictions_scores\n        ]\n        return predictions\n\n\nmodel_library = modelkit.ModelLibrary(models=[Tokenizer, Vectorizer, Classifier])\nclassifier = model_library.get(\"imdb_classifier\")\nprediction = classifier.predict({\"text\": \"I love the main character\"})\nprint(prediction.label)\n</code></pre>"},{"location":"examples/nlp_sentiment/tokenizer/","title":"Tokenizer","text":"<p>In this section, we will cover the basics of <code>modelkit</code>'s API, and use spaCy as tokenizer for our NLP pipeline.</p>"},{"location":"examples/nlp_sentiment/tokenizer/#installation","title":"Installation","text":"<p>Once you have set up a fresh python environment, let's install <code>modelkit</code>, <code>spacy</code> and grab the small english model.</p> <pre><code>pip install modelkit spacy\npython -m spacy download en_core_web_sm\n</code></pre>"},{"location":"examples/nlp_sentiment/tokenizer/#simple-model-predict","title":"Simple Model predict","text":"<p>To define a modelkit <code>Model</code>, you need to:</p> <ul> <li>create a class inheriting from <code>modelkit.Model</code> </li> <li>implement a <code>_predict</code> method</li> </ul> <p>To begin with, let's create a minimal tokenizer:</p> <pre><code>import modelkit\n\n\nclass Tokenizer(modelkit.Model):\n    def _predict(self, text):\n        return text.split()\n</code></pre> <p>That's it! It is very minimal, but sufficient to define a modelkit <code>Model</code>.</p> <p>You can now instantiate and call the <code>Model</code>:</p> <pre><code>tokenizer = Tokenizer()\n\ntokenizer.predict(\"I am a Data Scientist from Amiens, France\")\n</code></pre> Other ways to call predict <p>It is also possible to get predictions for batches (lists of items):</p> <pre><code>tokenizer.predict_batch([\n    \"I am a Data Scientist from Amiens, France\", \n    \"And I use modelkit\"\n])\n</code></pre> <p>or call predict as a generator:</p> <pre><code>for prediction in tokenizer.predict_gen((\"I am a Data Scientist from Amiens, France\",)):\n    print(prediction)\n</code></pre>"},{"location":"examples/nlp_sentiment/tokenizer/#complex-model-initialization","title":"Complex <code>Model</code> initialization","text":"<p>Let's now use spaCy to get closer to a production-ready tokenizer.</p> <p>This will also help demonstrate additional Modelkit features.</p> <pre><code>import modelkit\nimport spacy\n\nclass Tokenizer(modelkit.Model):\n    def _load(self):\n        self.nlp = spacy.load(\n            \"en_core_web_sm\",\n            disable=[\n                \"parser\",\n                \"ner\",\n                \"tagger\",\n                \"lemmatizer\",\n                \"tok2vec\",\n                \"attribute_ruler\",\n            ],\n        )\n\n    def _predict(self, text):\n        text = \" \".join(text.replace(\"&lt;br\", \"\").replace(\"/&gt;\", \"\").split())\n        return [\n            t.lower_\n            for t in self.nlp(text)  #\u00a0self.nlp is guaranteed to be initialized\n            if t.is_ascii and len(t) &gt; 1 and not (t.is_punct or t.is_stop or t.is_digit)\n        ]\n</code></pre> <p>We implement a <code>_load</code> method, which is where any asset, artifact, and other complex model attributes are created. </p> <p>This method will be called exactly once in the lifetime of the <code>Model</code> object. </p> <p>We define the spacy pipeline in the <code>_load</code> method (as opposed to <code>_predict</code> or the <code>__init__</code> methods), because it allows your model to benefit from advanced <code>modelkit</code> features such as lazy loading and dependency management.</p> <p>Since we will only be using the tokenizer and not the many other cool spacy features, let's not forget to disable them.</p> <p>We can instantiate the model and get predictions as before: <pre><code>tokenizer = Tokenizer() # _load is called\ntokenizer.predict(\"spaCy is a great lib for NLP \ud83d\ude00\")\n# ['spacy', 'great', 'lib', 'nlp']\n</code></pre></p>"},{"location":"examples/nlp_sentiment/tokenizer/#batch-computation","title":"Batch computation","text":"<p>So far, we have only implemented the <code>_predict</code> method, which tokenizes items one by one. </p> <p>In many instances, however, models will be called with many items at once, and we can leverage vectorization for speedups. This is particularly true when using other frameworks (Numpy, spaCy, Tensorflow, PyTorch etc.), or distant calls (TF Serving, database accesses etc.).</p> <p>To leverage batching, modelkit allows you to define a <code>_predict_batch</code> method to process lists of items, and thus kill multiple birds with one stone.</p> <p>Here we use spaCy's <code>pipe</code> method to tokenize items in batch:</p> <pre><code>import modelkit\nimport spacy\n\n\nclass Tokenizer(modelkit.Model):\n    def _load(self):\n        self.nlp = spacy.load(\n            \"en_core_web_sm\",\n            disable=[\n                \"parser\",\n                \"ner\",\n                \"tagger\",\n                \"lemmatizer\",\n                \"tok2vec\",\n                \"attribute_ruler\",\n            ],\n        )\n\n    def _predict_batch(self, texts):\n        texts = [\n            \" \".join(text.replace(\"&lt;br\", \"\").replace(\"/&gt;\", \"\").split())\n            for text in texts\n        ]\n        return [\n            [\n                t.lower_\n                for t in text\n                if t.is_ascii\n                and len(t) &gt; 1\n                and not (t.is_punct or t.is_stop or t.is_digit)\n            ]\n            for text in self.nlp.pipe(texts, batch_size=len(texts))\n        ]\n</code></pre> <p>Compared to the implementation with a <code>_predict</code> call, the time needed to tokenize batches of data is divided by 2.</p> <p>For example, using ipython's <code>timeit</code> to process a list of a 100 strings: <pre><code>%timeit [Tokenizer().predict(\"spaCy is a great lib for NLP\") for _ in range(100)]\n# 11.1 ms \u00b1 203 \u00b5s per loop on a 2020 Macbook Pro.\n\n%timeit Tokenizer().predict_batch([\"spaCy is a great lib for NLP] * 100, batch_size=64)\n# 5.5 ms \u00b1 105 \u00b5s per loop on a 2020 Macbook Pro.\n</code></pre></p> Caching predictions <p><code>modelkit</code> also allows you to use prediction caching (using Redis, or Python native caches) to improve computation times when the same items are seen over and over</p>"},{"location":"examples/nlp_sentiment/tokenizer/#additional-features","title":"Additional features","text":""},{"location":"examples/nlp_sentiment/tokenizer/#tests","title":"Tests","text":"<p>So far the tokenizer is relatively simple, but it is always useful to test your code.</p> <p><code>modelkit</code> encourages you to add test cases alongside the <code>Model</code> class definition to ensure that it behaves as intended, and serve as documentation.</p> <pre><code>import modelkit\nimport spacy\n\n\nclass Tokenizer(modelkit.Model):\n    TEST_CASES = [\n        {\"item\": \"\", \"result\": []},\n        {\"item\": \"NLP 101\", \"result\": [\"nlp\"]},\n        {\n            \"item\": \"I'm loving the spaCy 101 course !!!\u00f9*`^@\ud83d\ude00\",\n            \"result\": [\"loving\", \"spacy\", \"course\"],\n        },\n        {\n            \"item\": \"&lt;br/&gt;prepare things for IMDB&lt;br/&gt;\",\n            \"result\": [\"prepare\", \"things\", \"imdb\"],\n        },\n        {\n            \"item\": \"&lt;br/&gt;a b c data&lt;br/&gt;      e scientist\",\n            \"result\": [\"data\", \"scientist\", \"failing\", \"test\"],\n        },  # fails as intended\n    ]\n\n    def _load(self):\n        self.nlp = spacy.load(\n            \"en_core_web_sm\",\n            disable=[\n                \"parser\",\n                \"ner\",\n                \"tagger\",\n                \"lemmatizer\",\n                \"tok2vec\",\n                \"attribute_ruler\",\n            ],\n        )\n\n    def _predict_batch(self, texts):\n        texts = [\n            \" \".join(text.replace(\"&lt;br\", \"\").replace(\"/&gt;\", \"\").split())\n            for text in texts\n        ]\n        return [\n            [\n                t.lower_\n                for t in text\n                if t.is_ascii\n                and len(t) &gt; 1\n                and not (t.is_punct or t.is_stop or t.is_digit)\n            ]\n            for text in self.nlp.pipe(texts, batch_size=len(texts))\n        ]\n</code></pre> <p>You can run these test cases in the interactive programming tool of your choice (e.g. <code>ipython</code>, <code>jupyter</code> etc.) using the <code>test</code> method:</p> <pre><code>Tokenizer().test()\n# TEST 1: SUCCESS\n# TEST 2: SUCCESS\n# TEST 3: SUCCESS\n# TEST 4: SUCCESS\n# TEST 5: FAILED test failed on item\n# item = '&lt;br/&gt;a b c data&lt;br/&gt;      e scientist'                                               \n# expected = list instance                                                                     \n# result = list instance                                                                       \n</code></pre> Run using pytest <p>It is also possible to automatically test all models using the <code>pytest</code> integration, using the Modelkit autotesting fixture.</p> <p>Woops, seems like we need to fix the last test!</p>"},{"location":"examples/nlp_sentiment/tokenizer/#input-and-output-specification","title":"Input and output specification","text":"<p>It is good practice to specify inputs and outputs of models in production code</p> <p>This allows calls to be validated, thus ensuring consistency between calls, dependencies, services, and raising alerts when Models are not called as expected.</p> <p>This is also good for documentation, to understand how to use a given model, and during development to benefit from static type checking (e.g. with mypy).</p> <p><code>modelkit</code> allows you to define the expected input and output types of your model by subclassing <code>Model[input_type, output_type]</code>, where <code>input_type</code> and <code>output_type</code> can be standard Python types, dataclasses, or complex pydantic models.</p> <p>Let's add specification our Tokenizer to conclude this first part:</p> <pre><code>from typing import List\n\nimport modelkit\nimport spacy\n\n\nclass Tokenizer(modelkit.Model[str, List[str]]):\n    TEST_CASES = [\n        {\"item\": \"\", \"result\": []},\n        {\"item\": \"NLP 101\", \"result\": [\"nlp\"]},\n        {\n            \"item\": \"I'm loving the spaCy 101 course !!!\u00f9*`^@\ud83d\ude00\",\n            \"result\": [\"loving\", \"spacy\", \"course\"],\n        },\n        {\n            \"item\": \"&lt;br/&gt;prepare things for IMDB&lt;br/&gt;\",\n            \"result\": [\"prepare\", \"things\", \"imdb\"],\n        },\n        {\n            \"item\": \"&lt;br/&gt;a b c data&lt;br/&gt;      e scientist\",\n            \"result\": [\"data\", \"scientist\", \"failing\", \"test\"],\n        },  # fails as intended\n    ]\n\n    def _load(self):\n        self.nlp = spacy.load(\n            \"en_core_web_sm\",\n            disable=[\n                \"parser\",\n                \"ner\",\n                \"tagger\",\n                \"lemmatizer\",\n                \"tok2vec\",\n                \"attribute_ruler\",\n            ],\n        )\n\n    def _predict_batch(self, texts):\n        texts = [\n            \" \".join(text.replace(\"&lt;br\", \"\").replace(\"/&gt;\", \"\").split())\n            for text in texts\n        ]\n        return [\n            [\n                t.lower_\n                for t in text\n                if t.is_ascii\n                and len(t) &gt; 1\n                and not (t.is_punct or t.is_stop or t.is_digit)\n            ]\n            for text in self.nlp.pipe(texts, batch_size=len(texts))\n        ]\n</code></pre> <p>Calling the model with an unexpected type will raise a Modelkit <code>ItemValidationException</code>:</p> <pre><code>Tokenizer().predict([1, 2, 3, 4])\n</code></pre> <p>And <code>mypy</code> will raise errors if it encounters calls that are not correct: <pre><code>result : int = Tokenizer().predict(\"some text\")\n</code></pre></p>"},{"location":"examples/nlp_sentiment/tokenizer/#conclusion","title":"Conclusion","text":"<p>That's it! </p> <p>In this <code>modelkit</code> introduction, you have learned:</p> <ul> <li>How to create a basic <code>Model</code> by inheriting from <code>modelkit.Model</code> and implementing a <code>_predict</code> method</li> <li>How to correctly load artefacts/assets by overriding the <code>_load</code> method</li> <li>How to leverage batch computing to speed up execution by implementing a <code>_predict_batch</code> method</li> <li>How to add tests to ensure everything works as intended using <code>TEST_CASES</code> right in your model definition</li> <li>How to add specification to your model's inputs and outputs using <code>modelkit.Model[input_type, output_type]</code></li> </ul>"},{"location":"examples/nlp_sentiment/tokenizer/#final-tokenizer-code","title":"Final Tokenizer code","text":"<pre><code>from typing import List\n\nimport modelkit\nimport spacy\n\n\nclass Tokenizer(modelkit.Model[str, List[str]]):\n    CONFIGURATIONS = {\"imdb_tokenizer\": {}}\n    TEST_CASES = [\n        {\"item\": \"\", \"result\": []},\n        {\"item\": \"NLP 101\", \"result\": [\"nlp\"]},\n        {\n            \"item\": \"I'm loving the spaCy 101 course !!!\u00f9*`^@\ud83d\ude00\",\n            \"result\": [\"loving\", \"spacy\", \"course\"],\n        },\n        {\n            \"item\": \"&lt;br/&gt;prepare things for IMDB&lt;br/&gt;\",\n            \"result\": [\"prepare\", \"things\", \"imdb\"],\n        },\n        {\n            \"item\": \"&lt;br/&gt;a b c data&lt;br/&gt;      e scientist\",\n            \"result\": [\"data\", \"scientist\"],\n        },\n    ]\n\n    def _load(self):\n        self.nlp = spacy.load(\n            \"en_core_web_sm\",\n            disable=[\n                \"parser\",\n                \"ner\",\n                \"tagger\",\n                \"lemmatizer\",\n                \"tok2vec\",\n                \"attribute_ruler\",\n            ],\n        )\n\n    def _predict_batch(self, texts):\n        texts = [\n            \" \".join(text.replace(\"&lt;br\", \"\").replace(\"/&gt;\", \"\").split())\n            for text in texts\n        ]\n        return [\n            [\n                t.lower_\n                for t in text\n                if t.is_ascii\n                and len(t) &gt; 1\n                and not (t.is_punct or t.is_stop or t.is_digit)\n            ]\n            for text in self.nlp.pipe(texts, batch_size=len(texts))\n        ]\n</code></pre> <p>As you may have seen, there is a <code>CONFIGURATIONS</code> map in the class definition, we will cover it in the next section.</p>"},{"location":"examples/nlp_sentiment/vectorizer/","title":"Vectorizer","text":"<p>In this section, we will fit and implement a custom text vectorizer based on the sentiment analysis IMDB dataset.</p> <p>It will be the opportunity to go through modelkit's assets management basics, learn how to fetch artifacts, and get deeper into its API.</p>"},{"location":"examples/nlp_sentiment/vectorizer/#installation","title":"Installation","text":"<p>First, let's download the IMDB reviews dataset:</p> <pre><code># download the remote archive\ncurl https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz --output imdb.tar.gz\n# untar it\ntar -xvf imdb.tar.gz\n# remove the unsupervised directory we will not be using\nrm -rf aclImdb/train/unsup\n</code></pre> <p>The different files consist in text reviews left by IMDB users, each file corresponding to one review.</p> <p>The dataset is organized with train and test directories, containing positive and negative examples in subfolders corresponding to the target classes.</p>"},{"location":"examples/nlp_sentiment/vectorizer/#creating-an-asset-file","title":"Creating an asset file","text":""},{"location":"examples/nlp_sentiment/vectorizer/#fitting","title":"Fitting","text":"<p>First thing first, let's define a helper function to read through the training set.</p> <p>We will only be using generators to avoid loading the entire dataset in memory, (and later use <code>Model.predict_gen</code> to process them).</p> <pre><code>import glob\nfrom typing import Generator\n\ndef read_dataset(path: str) -&gt; Generator[str, None, None]:\n    for review in glob.glob(os.path.join(path, \"*.txt\")):\n        with open(review, 'r', encoding='utf-8') as f:\n            yield f.read()\n</code></pre> <p>We need to tokenize our dataset before fitting a Scikit-Learn <code>TfidfVectorizer</code>,</p> <p>Although sklearn includes a tokenizer, we will be using the one we implemented in the first section.</p> <pre><code>import itertools\nimport os\n\ntraining_set = itertools.chain(\n    read_dataset(os.path.join(\"aclImdb\", \"train\", \"pos\")),\n    read_dataset(os.path.join(\"aclImdb\", \"train\", \"neg\")),\n)\ntokenized_set = Tokenizer().predict_gen(training_set)\n</code></pre> <p>By using generators and the <code>predict_gen</code> method, we can to read huge texts corpora without filling up our memory.</p> <p>Each review will be tokenized one by one, but as we discussed we can also speed up execution by setting a <code>batch_size</code> greater than one, to process these reviews batch by batch.</p> <pre><code># here, an example with a 64 batch size\ntokenized_set = Tokenizer().predict_gen(training_set, batch_size=64)\n</code></pre> <p>We are all set! Let's fit our <code>TfidfVectorizer</code> using the <code>tokenized_set</code>, and disabling the embedded tokenizer.</p> <pre><code>from sklearn.feature_extraction.text import TfidfVectorizer\n\nvectorizer = TfidfVectorizer(\n    tokenizer=lambda x: x, lowercase=False, max_df=0.95, min_df=0.01\n).fit(tokenized_set)\n</code></pre> <p>We now need to save the vocabulary we just fitted to disk, before writing our on vectorizer <code>Model</code>.</p> <p>Of course, one could serialize the <code>TfidfVectorizer</code>, but there are many reasons why we would not want this in production code:</p> <ul> <li><code>TfidfVectorizer</code> is only used to build the vocabulary using neat features such as min/max document frequencies, which are no longer useful during inference</li> <li>it requires a heavy code dependency: <code>scikit-learn</code>, only for vectorization</li> <li>pickling scikit-learn models come with some tricky issues relative to dependency, version and security</li> </ul> <p>It is also common practice to separate the research / training phase, from the production phase.</p> <p>As a result, we will be implementing our own vectorizer for production using <code>modelkit</code>, based on the vocabulary created with scikit-learn's <code>TfidfVectorizer</code>.</p> <p>We just need to write a list of strings to disk:</p> <pre><code># we only keep strings from the vocabulary\n# we will be using our own str -&gt; int mapping\nvocabulary = next(zip(*sorted(vectorizer.vocabulary_.items(), key=lambda x: x[1])))\nwith open(\"vocabulary.txt\", \"w\", encoding=\"utf-8\") as f:\n    for row in vocabulary:\n        f.write(row + \"\\n\")\n</code></pre>"},{"location":"examples/nlp_sentiment/vectorizer/#using-the-file-in-a-model","title":"Using the file in a Model","text":"<p>In this subsection, you will learn how to leverage the basics of Modelkit's Assets management.</p> <p>Note</p> <p>Most of the features (assets remote storage, updates, versioning, etc.) will not be adressed in this tutorial, but are the way to go when things get serious.</p> <p>First, let's implement our <code>Vectorizer</code> as a <code>modelkit.Model</code> which loads the <code>vocabulary.txt</code> file we just created:</p> <pre><code>import modelkit\n\nfrom typing import List\n\n\nclass Vectorizer(modelkit.Model):\n    CONFIGURATIONS = {\"imdb_vectorizer\": {\"asset\": \"vocabulary.txt\"}}\n\n    def _load(self):\n        self.vocabulary = {}\n        with open(self.asset_path, \"r\", encoding=\"utf-8\") as f:\n            for i, k in enumerate(f):\n                self.vocabulary[k.strip()] = i\n</code></pre> <p>Here, we define a <code>CONFIGURATIONS</code> class attribute, which lets <code>modelkit</code> know how to find the vocabulary. <code>vocabulary.txt</code> will be found in the current working directory, and its absolute path written to the <code>Vectorizer.asset_path</code> attribute before <code>_load</code> is called.</p> Remote assets <p>When assets are stored remotely, the remote versioned asset is retrieved and cached on the local disk before the <code>_load</code> method is called, and its local path is set in the <code>self.asset_path</code> attribute.</p> <p><code>modelkit</code> guarantees that whenever <code>_load</code> is called, the file is present and its absolute path is written to <code>Model.asset_path</code>.</p> What can be an asset <p>An asset can be anything (a file, a directory), and the user is responsible for defining the loading logic in <code>_load</code>.  You can refer to it with a relative path (which will be relative to the current working directeory), an absolute path, or a remote asset specification.</p> <p>In this case, it is rather straighforward: <code>self.asset_path</code> directly points to <code>vocabulary.txt</code>.</p> <p>In addition, the <code>imdb_vectorizer</code> configuration key can now be used to refer to the <code>Vectorizer</code> in a <code>ModelLibrary</code> object. This allows you to define multiple <code>Vectorizer</code> objects with different vocabularies, without rewriting the prediction logic.</p>"},{"location":"examples/nlp_sentiment/vectorizer/#prediction","title":"Prediction","text":"<p>Now let us add a more complex prediction logic and input specification:</p> <pre><code>import numpy as np\nimport modelkit\n\nfrom typing import List\n\n\nclass Vectorizer(modelkit.Model[List[str], List[int]]):\n    CONFIGURATIONS = {\"imdb_vectorizer\": {\"asset\": \"vocabulary.txt\"}}\n\n    def _load(self):\n        self.vocabulary = {}\n        with open(self.asset_path, \"r\", encoding=\"utf-8\") as f:\n            for i, k in enumerate(f):\n                self.vocabulary[k.strip()] = i + 2\n        self._vectorizer = np.vectorize(lambda x: self.vocabulary.get(x, 1))\n\n    def _predict(self, tokens, length=None, drop_oov=True):\n        vectorized = (\n            np.array(self._vectorizer(tokens), dtype=np.int)\n            if tokens\n            else np.array([], dtype=int)\n        )\n        if drop_oov and len(vectorized):\n            vectorized = np.delete(vectorized, vectorized == 1)\n        if not length:\n            return vectorized.tolist()\n        result = np.zeros(length)\n        vectorized = vectorized[:length]\n        result[: len(vectorized)] = vectorized\n        return result.tolist()\n</code></pre> <p>We add several advanced features, first, to deal with out of vocabulary or padding tokens, we reserve the following integers:</p> <ul> <li><code>0</code> for padding</li> <li><code>1</code> for the unknown token (out-of-vocabulary words)</li> <li><code>2+i</code> for known vocabulary words</li> </ul> <p>We use <code>np.vectorize</code> to map a tokens list to an indices list, which we store in the <code>_vectorize</code> attribute in the <code>_load</code>.</p> <p>We also add keyword arguments to the <code>_predict</code>:  <code>length</code> and <code>drop_oov</code>. These can be used during prediction as well, and would be passed to <code>_predict</code> or <code>_predict_batch</code>:</p> <pre><code>vectorizer = modelkit.load_model(\"imdb_vectorizer\", models=Vectorizer)\nvectorizer.predict(item, length=10, drop_oov=False)\nvectorizer.predict_batch(items, length=10, drop_oov=False)\nvectorizer.predict_gen(items, length=10, drop_oov=False)\n</code></pre>"},{"location":"examples/nlp_sentiment/vectorizer/#test-cases","title":"Test cases","text":"<p>Now let us add test cases too. The only trick here is that we have to add the information about our new keyword arguments when we want to test different values.</p> <p>To do so, we use the <code>keyword_args</code> field in the test cases:</p> <pre><code>class Vectorizer(modelkit.Model[List[str], List[int]]):\n    ...\n\n    TEST_CASES = [\n        {\"item\": [], \"result\": []},\n        {\"item\": [], \"keyword_args\": {\"length\": 10}, \"result\": [0] * 10},\n        {\"item\": [\"movie\"], \"result\": [888]},\n        {\"item\": [\"unknown_token\"], \"result\": []},\n        {\n            \"item\": [\"unknown_token\"],\n            \"keyword_args\": {\"drop_oov\": False},\n            \"result\": [1],\n        },\n        {\"item\": [\"movie\", \"unknown_token\", \"scenes\"], \"result\": [888, 1156]},\n        {\n            \"item\": [\"movie\", \"unknown_token\", \"scenes\"],\n            \"keyword_args\": {\"drop_oov\": False},\n            \"result\": [888, 1, 1156],\n        },\n        {\n            \"item\": [\"movie\", \"unknown_token\", \"scenes\"],\n            \"keyword_args\": {\"length\": 10},\n            \"result\": [888, 1156, 0, 0, 0, 0, 0, 0, 0, 0],\n        },\n        {\n            \"item\": [\"movie\", \"unknown_token\", \"scenes\"],\n            \"keyword_args\": {\"length\": 10, \"drop_oov\": False},\n            \"result\": [888, 1, 1156, 0, 0, 0, 0, 0, 0, 0],\n        },\n    ]\n    ...\n</code></pre>"},{"location":"examples/nlp_sentiment/vectorizer/#final-vectorizer","title":"Final vectorizer","text":"<p>Putting it all together, we obtain:</p> <pre><code>import modelkit\nimport numpy as np\n\nfrom typing import List\n\n\nclass Vectorizer(modelkit.Model[List[str], List[int]]):\n    CONFIGURATIONS = {\"imdb_vectorizer\": {\"asset\": \"vocabulary.txt\"}}\n    TEST_CASES = [\n        {\"item\": [], \"result\": []},\n        {\"item\": [], \"keyword_args\": {\"length\": 10}, \"result\": [0] * 10},\n        {\"item\": [\"movie\"], \"result\": [888]},\n        {\"item\": [\"unknown_token\"], \"result\": []},\n        {\n            \"item\": [\"unknown_token\"],\n            \"keyword_args\": {\"drop_oov\": False},\n            \"result\": [1],\n        },\n        {\"item\": [\"movie\", \"unknown_token\", \"scenes\"], \"result\": [888, 1156]},\n        {\n            \"item\": [\"movie\", \"unknown_token\", \"scenes\"],\n            \"keyword_args\": {\"drop_oov\": False},\n            \"result\": [888, 1, 1156],\n        },\n        {\n            \"item\": [\"movie\", \"unknown_token\", \"scenes\"],\n            \"keyword_args\": {\"length\": 10},\n            \"result\": [888, 1156, 0, 0, 0, 0, 0, 0, 0, 0],\n        },\n        {\n            \"item\": [\"movie\", \"unknown_token\", \"scenes\"],\n            \"keyword_args\": {\"length\": 10, \"drop_oov\": False},\n            \"result\": [888, 1, 1156, 0, 0, 0, 0, 0, 0, 0],\n        },\n    ]\n\n    def _load(self):\n        self.vocabulary = {}\n        with open(self.asset_path, \"r\", encoding=\"utf-8\") as f:\n            for i, k in enumerate(f):\n                self.vocabulary[k.strip()] = i + 2\n        self._vectorizer = np.vectorize(lambda x: self.vocabulary.get(x, 1))\n\n    def _predict(self, tokens, length=None, drop_oov=True):\n        vectorized = (\n            np.array(self._vectorizer(tokens), dtype=np.int)\n            if tokens\n            else np.array([], dtype=int)\n        )\n        if drop_oov and len(vectorized):\n            vectorized = np.delete(vectorized, vectorized == 1)\n        if not length:\n            return vectorized.tolist()\n        result = np.zeros(length)\n        vectorized = vectorized[:length]\n        result[: len(vectorized)] = vectorized\n        return result.tolist()\n</code></pre> <p>In the next section, we will train a classifier using the Tokenizer and Vectorizer models we just created. This will show us how to compose and store models in <code>modelkit</code>.</p>"},{"location":"library/caching/","title":"Caching","text":""},{"location":"library/caching/#prediction-caching","title":"Prediction caching","text":"<p>It is possible to use a redis caching mechanism to cache all calls to predict for a <code>ModelLibrary</code> using the <code>enable_redis_cache</code>, <code>cache_host</code>, and <code>cache_port</code> settings of the <code>LibrarySettings</code>. This has to be enabled for each model by setting the <code>cache_predictions</code> model setting to <code>True</code>.</p> <p>The caching works on individual items, before making a prediction with the methods in the <code>Model</code> class, it will attempt to see if an available prediction is already available in the cache.</p> <p>Predictions in the cache are keyed by a hash of the passed item alongside the key of the model (the key used in the configuration of the model).</p> <p>When a prediction on a batch of items is requested, the <code>Model</code> will sieve through each item and attempt to find cached predictions for each. It will therefore only recompute predictions for the select items that do not appear in the cache.</p>"},{"location":"library/lazy_loading/","title":"Lazy loading","text":"<p>Usually, all model assets are loaded as soon as the <code>ModelLibrary</code> is instantiated. Sometimes this is not desirable, notably when using PySpark.</p> <p>Thus, when <code>lazy_loading=True</code> the <code>ModelLibrary</code> tries to delay the loading and deserialization of the assets as much as possible. You can also set this behavior by setting <code>MODELKIT_LAZY_LOADING=True</code> in your environment.</p> <p>Specifically:</p> <ul> <li>When the <code>ModelLibrary</code> is instantiated nothing really happens: the <code>Model</code> object is instantiated without deserializing the asset.</li> <li>When <code>ModelLibrary.get</code> is called the first time, the <code>Model</code>'s asset is downloaded (via <code>ModelLibrary._load</code>) to a local directory and deserialized.</li> </ul> <p>It is also possible to explicitly ask the <code>ModelLibrary</code> to load all <code>required_models</code> at once by calling <code>ModelLibrary.preload</code>.</p>"},{"location":"library/model_library/","title":"Model library","text":"<p>The <code>ModelLibrary</code> is the primary object that provides predictions from  models.</p> <p><code>ModelLibrary</code> objects can have a number of settings, passed as a dictionary upon initialization <code>ModelLibrary(required_models = ..., settings = ...)</code>. These parameters are exploited by the ModelLibrary directly and set as the <code>service_settings</code> attribute of <code>Model</code> objects.</p> <p>Main arguments:</p> <ul> <li><code>models</code> a module, <code>Model</code> (or list of either) which is used to find configurations   of the models to be loaded</li> <li><code>configuration</code> allows you to provide an explicit configuration to override the ones present in the <code>Model.CONFIGURATIONS</code> attributes.</li> <li><code>required_models</code> a list of models to load by the library. This allows you to restrict the models that will actually be loaded into memory. By default all models from <code>models</code> are loaded (<code>required_models=None</code>), pass the empty list to not load any models (or use the lazy mode). Names in this list have to be defined in the configurations of the models passed via <code>models</code>. You can pass a dictionary to override the asset for each model.</li> </ul> <p>Additionally, the <code>ModelLibrary</code> takes a <code>settings</code> keyword argument which allows you to provide advanced settings:</p> <p><code>Model</code> instance to be created for the required models. This is useful to download the assets for example with TF serving</p> <ul> <li><code>lazy_loading</code>: when True, this will cause the assets to be loaded lazily.  This is useful for pyspark jobs with model object that are not serializable</li> <li><code>enable_tf_serving</code>, <code>tf_serving_port</code>, <code>tf_serving_host</code>: Set parameters related to the serving of TF models (see here).</li> <li><code>assetsmanager_settings</code>: Parameters passed to the <code>assets.manager.AssetsManager</code></li> <li><code>override_assets_dir</code>: Specify an alternative assets directory from which the prediction service will try to use assets before falling back to the normal assets directory. It is used to test new assets without having to push them in the main storage.</li> </ul>"},{"location":"library/overview/","title":"Overview","text":"<p>The main concepts in <code>modelkit</code> are <code>Model</code> and the <code>ModelLibrary</code>.</p> <p>The <code>ModelLibrary</code> instantiates and configures <code>Model</code> objects and keeps track of them during execution. </p> <p><code>Model</code> objects can then be requested via <code>ModelLibrary.get</code>, and used to make predictions via <code>Model.predict</code>. The ML logic is written in each <code>Model</code>'s <code>predict</code> functions, typically inside a module.</p>"},{"location":"library/overview/#model-and-modellibrary","title":"Model and ModelLibrary","text":"<p>The normal way to use <code>modelkit</code> models is by creating models by subclassing the <code>modelkit.Model</code> class, and adding a configuration, then creating a ModelLibrary by instantiating a <code>ModelLibrary</code> with a set of models.</p> <pre><code>from modelkit import ModelLibrary, Model\n\n# Create a Model subclass\nclass MyModel(Model):\n    # Give it a name\n    CONFIGURATIONS = {\"my_favorite_model\": {}}\n\n    # Write some prediction logic\n    def _predict(self, item):\n        return item\n\n\n# Create the model library\nlibrary = ModelLibrary(models=MyModel)\n\n# Get the model\nmodel = library.get(\"my_favorite_model\")\n\n# Get predictions\nmodel(\"hello world\") # returns hello world\n</code></pre> <p>In the tutorial you will learn that:</p> <ul> <li>Models can have an asset linked to them, to store parameters, weights, or anything really. This asset is loaded and deserialized when the <code>ModelLibrary</code> instantiates the object.</li> <li>Models can depend on other models and share objects in memory (in particular, they can share assets). Only the minimal subset of models is loaded when a given model is required.</li> <li>Model inputs and outputs can be systematically validated using pydantic</li> <li>Models can implement vectorized logic to make faster predictions.</li> <li>Models can implement asynchronous logic and be called either way.</li> <li>Models can serve Tensorflow models conveniently</li> </ul>"},{"location":"library/models/asynchronous_models/","title":"Async","text":"<p>Although we have only describe synchronous models so far, <code>modelkit</code> allows you to write asynchronous code in <code>Model</code> objects and use the exact same functionality as described.</p> <p>To do so, simply subclass the <code>modelkit.AsyncModel</code> instead of <code>Model</code>.</p> <pre><code>class SomeAsyncModel(AsyncModel):\n    CONFIGURATIONS = {\"async_model\": {}}\n\n    async def _predict(self, item, **kwargs):\n        await asyncio.sleep(0)\n        return item\n</code></pre> <p>Now, it is required to write the <code>_predict</code> or <code>_predict_batch</code> methods with <code>async def</code>, and you can use <code>await</code> expressions.</p> <p>Similarly, the <code>predict</code> and <code>predict_batch</code> methods become async, and <code>predict_gen</code> is an async generator:</p> <pre><code>m = SomeAsyncModel()\n\nawait m.predict(...)\nawait m.predict_batch(...)\n\nasync for res in m.predict_gen(...):\n    ...\n</code></pre>"},{"location":"library/models/asynchronous_models/#async-and-sync-composition","title":"Async and sync composition","text":"<p>To make it easy to have both synchronous and asynchronous models in the same prediction call stack, <code>modelkit</code> attempts to detect which context it is in and tries to make asynchronous models available even in synchronous contexts.</p>"},{"location":"library/models/asynchronous_models/#sync-in-async","title":"Sync in async","text":"<p>If you have an asynchronous <code>Model</code> that depends on a synchronous model, there is nothing to do, you can simply call it as usual <pre><code>model_a (async) -depends-on-&gt; model_b\n</code></pre></p> <p>In <code>model_b._predict</code> this causes no issues</p> <pre><code>    async def _predict(self, item):\n        ...\n        something = self.model_dependencies[\"model_b\"].predict(...)\n        ...\n        return ...\n</code></pre>"},{"location":"library/models/asynchronous_models/#async-in-sync","title":"Async in sync","text":"<p>The opposite situation wherein you call an asynchronous model in a synchronous context is more annoying:</p> <pre><code>model_a -depends-on-&gt; model_b (async) \n</code></pre> <p>Indeed, this code would be invalid since <code>predict</code> returns a coroutine <pre><code>    def _predict(self, item):\n        ...\n        something = self.model_dependencies[\"model_b\"].predict(...)\n        ...\n        return ...\n</code></pre></p> <p>To work around this, when <code>modelkit</code> encounters an <code>AsyncModel</code> in a synchronous context, it will wrap it in a <code>WrappedAsyncModel</code> that exposes \"syncified\" versions of the <code>predict</code> functions using asgiref.</p> <p>As a result, the <code>model_a</code> will have a different object in its dependency, making the following valid. <pre><code>    def _predict(self, item):\n        ...\n        assert isinstance(self.model_dependencies[\"model_b\"], WrappedAsyncModel)\n        something = self.model_dependencies[\"model_b\"].predict(...)\n        ...\n        return ...\n</code></pre></p> <p>TL;DR, if you want to use asynchronous logic in the <code>modelkit</code> code, make sure that your dependencies do not have a <code>sync-with-async-dependency</code> in the chain, otherwise this may create issues.</p>"},{"location":"library/models/batching/","title":"Batching","text":"<p>Oftentimes, when using external libraries, it is much faster to compute predictions with batches of items. This is true with deep learning, but also when writing code with libraries like <code>numpy</code>. </p> <p>For this reason, it is possible to define <code>Model</code> with batched prediction code, by overriding <code>_predict_batch</code> instead of <code>_predict</code>:</p> <pre><code>class MyModel(Model):\n    def _predict(self, item):\n        return item\n    # OR \n    def _predict_batch(self, items):\n        return items\n</code></pre> <p>Whichever one you decide to implement, <code>modelkit</code> will still expose the same methods <code>predict</code>, <code>predict_batch</code> and <code>predict_gen</code>.</p> <p>Typically, one would first implement <code>_predict</code> to get the logic right, and later, if needed, implement <code>_predict_batch</code> to improve performance.</p> <p>Warning</p> <p>Don't override both <code>_predict</code> and <code>_predict_batch</code>. This will raise an error</p> Example <p>In this example, we implement a dummy <code>Model</code> that computes the position of the min in a list using <code>np.argmin</code>. </p> <p>In one version the code is not vectorized (it operates on a single item) and in the other one it is (a whole batched is processed at once).</p> <p>The vectorized version is ~50% faster <pre><code>import random\nimport timeit\n\nfrom modelkit.core.model import Model\nimport numpy as np\n\n# Create some data\ndata = []\nbase_item = list(range(100))\nfor _ in range(128):\n    random.shuffle(base_item)\n    data.append(list(base_item))\n\n# This model is not vectorized, `np.argmin`\n# will be called individually for each batch\nclass MinModel(Model):\n    def _predict(self, item):\n        return np.argmin(item)\n\n\nm = MinModel()\n\n# This model is vectorized, `np.argmin`\n# is called over a whole batch\nclass MinBatchedModel(Model):\n    def _predict_batch(self, items):\n        return np.argmin(items, axis=1)\n\n\nm_batched = MinBatchedModel()\n\n# They do return the same results\nassert m.predict_batch(data) == m_batched.predict_batch(data)\n\n\n# The batched model is ~50% slower\ntimeit.timeit(lambda: m.predict_batch(data), number=1000)\n# The batched model is ~50% slower\ntimeit.timeit(lambda: m_batched.predict_batch(data), number=1000)\n# Even more so with a larger batch size\ntimeit.timeit(lambda: m_batched.predict_batch(data, batch_size=128), number=1000)\n</code></pre></p>"},{"location":"library/models/batching/#controling-batch-size","title":"Controling batch size","text":"<p>The default batch size for the <code>Model</code> object is controlled by its <code>batch_size</code> attribute. It defaults to <code>None</code>, which means that <code>_predict_batch</code> will by default always get:</p> <ul> <li>a single, full length batch with all the items when called via <code>predict_batch</code></li> <li>as many batches of size one as there are items when called via <code>predict_gen</code></li> </ul> <p>It is possible to control the batch size for each call to <code>_predict_batch</code>, by using:</p> <pre><code>items = [1,2,3,4]\npredictions = model.predict_batch(items, batch_size=2)\nfor p in model.predict_gen(iter(items), batch_size=2):\n    ...\n# predictions will be computed in batches of two\n</code></pre> <p>This is useful to avoid computing batches that are too large and may take up too much memory.</p> <p>Note that, although <code>modelkit</code> will attempt to build batches of even size, this is not always the case:</p> <ul> <li>remaining items if you request 10 predictions with a batch size of 3, the last batch will only contain one.</li> <li>caching when caching, <code>modelkit</code> will yield if sufficiently many predictions can be fetched in the cache, and compute the rest, wich will lead to smaller batches than expected.</li> </ul> <p>If you do need to access the number of items in a batch, use <code>len(items)</code> inside the <code>_predict_batch</code>. If you need to make sure that it is contant, you will have to implement padding yourself.</p> <p>Batches</p> <p>When using <code>predict_gen</code> with a model with <code>_predict_batch</code> implemented, <code>modelkit</code> will construct batches, while still yielding items one by one.</p>"},{"location":"library/models/configuring_models/","title":"Configuring models","text":"<p>As models become more complicated they are attached to different assets or other models. We will need to instanciate them through the <code>ModelLibrary</code> object which will take care of all this for us.</p> <p>To do so, we have to configure our model: give it a name, and possibly assets, dependencies, adding test cases, etc.</p> <p>Models are made available to clients using <code>modelkit</code> by specifying them using the <code>CONFIGURATIONS</code> class attribute:</p> <pre><code>class SimpleModel(Model):\n    CONFIGURATIONS = {\n        \"simple\": {}\n    }\n    def _predict(self, item):\n        return \"something\"\n</code></pre> <p>Right now, we have only given it a name <code>\"simple\"</code> which makes the model available to other models via the <code>ModelLibrary</code>.</p> <p>The rest of the configuration is empty but we will add to it at the next section.</p> <p>Assuming that <code>SimpleModel</code> is defined in <code>my_module.my_models</code>, it is now accessible via:</p> <pre><code>from modelkit.core import ModelLibrary\nimport my_module.my_models\n\np = ModelLibrary(models=my_module.my_models)\nm = p.get(\"simple\")\n</code></pre> <p>See Organization for more information on how to organize your models.</p>"},{"location":"library/models/configuring_models/#model-settings","title":"Model settings","text":"<p>The simplest configuration options are <code>model_settings</code>:</p> <pre><code>from modelkit.core.model import Model\n\nclass SimpleModel(Model):\n    CONFIGURATIONS = {\n        \"simple\": {\"model_settings\": {\"value\" : \"something\"}},\n        \"simple2\": {\"model_settings\": {\"value\" : \"something2\"}}\n    }\n    def _predict(self, item):\n        return self.model_settings[\"value\"]\n</code></pre> <p>Now, there are two versions of the model available, <code>simple</code> and <code>simple2</code>:</p> <pre><code>from modelkit.core import ModelLibrary\n\np = ModelLibrary(models=SimpleModel)\nm = p.get(\"simple\")\nprint(m({}))\nm2 = p.get(\"simple2\")\nprint(m2({}))\n</code></pre> <p>It will print <code>\"something\"</code> and <code>\"something2\"</code>.</p>"},{"location":"library/models/configuring_models/#model-attributes","title":"Model attributes","text":"<p>In general, <code>Model</code> have several attributes set by the <code>ModelLibrary</code> when they are loaded:</p> <ul> <li><code>asset_path</code> the path to the asset set in the <code>Model</code>'s configuration</li> <li><code>configuration_key</code> the key of the model's configuration</li> <li><code>model_dependencies</code> a dictionary of <code>Model</code> dependencies</li> <li><code>model_settings</code> the <code>model_settings</code> as passed at initialization</li> <li><code>service_settings</code> the settings of the <code>ModelLibrary</code> that created the model</li> <li><code>batch_size</code> the batch size for the model: if <code>_predict_batch</code> is implemented it will default to getting batches of this size. It defaults to <code>None</code>, which means \"no batching\"</li> </ul>"},{"location":"library/models/configuring_models/#asset-class","title":"<code>Asset</code> class","text":"<p>It is sometimes useful for a given asset in memory to serve many different <code>Model</code> objects. It is possibly by using the <code>model_dependencies</code> to point to a parent <code>Model</code> that is the only one to load the asset via <code>_load</code>.</p> <p>In this case, we may not want the parent asset-bearing <code>Model</code> object to implement <code>predict</code> at all.</p> <p>This is what an <code>modelkit.core.model.Asset</code> is.</p> <p>Note</p> <p>In fact, it is defined the other way around: <code>Model</code>s are <code>Asset</code>s with a predict function, and thus <code>Model</code> inherits from <code>Asset</code>.</p> <p>Note</p> <p>There are two ways to use a data asset in a <code>Model</code>: either load it directly via its configuration and the <code>_load</code>, or package it in an <code>Asset</code> and use the deserialized object via model dependencies.</p>"},{"location":"library/models/model_with_load/","title":"Model loading","text":"<p>A model can implement a <code>_load</code> method that is called by <code>modelkit</code> either when loading the <code>model</code> through a <code>ModelLibrary</code> or when instantiating it. </p> <p>This allows you to load information from a files or folders stored locally (or retrieved from an object store). These can contain arbitrary files, folders, parameters, optimized data structures, or anything really.</p> <p><code>modelkit</code> ensures that it is only ever called once, regardless of how many times you want to use your model.</p> <p><code>modelkit</code> refers to these supporting files and directories used to load model as assets.</p>"},{"location":"library/models/model_with_load/#defining-a-model-asset","title":"Defining a model asset","text":"<p>The model asset is specified in the <code>CONFIGURATIONS</code> under the <code>asset</code> key: </p> <pre><code>class ModelWithAsset(Model):\n    CONFIGURATIONS = {\n        \"model_with_asset\": {\"asset\": \"some_file.txt\"}\n    }\n\n    def _load(self):\n        with open(self.asset_path) as f:\n            ...\n</code></pre> <p>When the <code>_load</code> method is called, the object will have an <code>asset_path</code> attribute that points to the absolute path of the asset locally. <code>modelkit</code> will ensure that the file is actually present before <code>_load</code> is called.</p> <p>The <code>_load</code> method is used to load the relevant information from the asset file(s).</p>"},{"location":"library/models/model_with_load/#asset-convention-and-resolution","title":"Asset convention and resolution","text":"<p>The string definition of an asset in the <code>CONFIGURATIONS</code> can refer to different things:</p> <ul> <li>an absolute path to a local file</li> <li>a relative path to a local file (relative to the current working directory)</li> <li>a relative path to a local file (relative to the \"assets directory\" a directory typically set with an environment variable <code>MODELKIT_ASSETS_DIR</code>)</li> <li>an asset specification which can refer to assets stored remotely</li> </ul> <p>We encourage you to use Asset specification with remote asset storage, in order to get all the power of <code>modelkit</code> asset system.</p>"},{"location":"library/models/model_with_load/#other-dependencies-usable-during-_load","title":"Other dependencies usable during <code>_load</code>","text":"<p>Whenever <code>_load</code> is called, <code>modelkit</code> ensures that all other dependencies are already loaded and ready to go. These include:</p> <ul> <li><code>asset_path</code> the path to the asset (which is guaranteed to be present when it is called)</li> <li><code>model_settings</code> as present in the <code>CONFIGURATIONS</code></li> <li><code>model_dependencies</code> are fully loaded</li> <li><code>service_settings</code> the settings of the <code>ModelLibrary</code> </li> </ul>"},{"location":"library/models/model_with_load/#_load-vs-__init__","title":"<code>_load</code> vs. <code>__init__</code>","text":"<p>Assets retrieval, dependencies management, are all handled for you by <code>modelkit.ModelLibrary</code>. For some features, it is necessary to instantiate the objects first, and only after resolve all assets and dependencies (e.g. in lazy mode, which is useful for Spark for example).</p> <p>As a result it is only guaranteed that attributes will be present when <code>_load</code> is called, rather than <code>__init__</code>. This is why it is not in general a good idea to override <code>__init__</code> (or instantiate the models without the help of a <code>ModelLibrary</code>. </p>"},{"location":"library/models/models_with_dependencies/","title":"Dependencies","text":"<p><code>modelkit</code> models are composable: a <code>Model</code> can depend on other <code>Model</code>s, and exploit their attributes and predictions.</p> <p>The <code>ModelLibrary</code> ensures that whenever <code>_load</code> or the <code>_predict_*</code> function are called, these models are loaded and present in the <code>model_dependencies</code> dictionary:</p> <p>For example your can set your model's configuration to have access to two other <code>Model</code> objects:</p> <pre><code>class SomeModel(Model):\n    CONFIGURATIONS = {\n        \"some_model\": {\n            \"model_dependencies\": {\n                \"sentence_piece_cleaner\",\n                \"sentence_piece_vectorizer\"\n            }\n        }\n    }\n    def _predict(self, item):\n        # The `model_dependencies` attribute contains fully loaded dependent\n        # models which can be used directly:\n        cleaned = self.models_dependencies[\"sentence_piece_cleaner\"].predict(item[\"text\"])\n        ...\n</code></pre>"},{"location":"library/models/models_with_dependencies/#renaming-dependencies","title":"Renaming dependencies","text":"<p>In addition, it is possible to rename dependencies on the fly by providing a mapping to <code>model_dependencies</code>. This is useful in order to keep the same <code>predict</code> code, even though dependencies have changed:</p> <pre><code>class SomeModel(Model):\n    CONFIGURATIONS = {\n        \"some_model\": {\n            \"model_dependencies\": {\n                \"cleaner\": \"sentence_piece_cleaner\",\n            }\n        },\n        \"some_model_2\": {\n            \"model_dependencies\": {\n                \"cleaner\": \"sentence_piece_cleaner_2\"\n            }\n        }\n    }\n\n    def _predict(self, item):\n        # Will call `sentence_piece_cleaner` in the `some_model` model and\n        # `sentence_piece_cleaner_2` in the `some_model_2` model\n        return self.model_dependencies[\"cleaner\"].predict(item)\n</code></pre> Renaming several dependencies <p>All your dependencies must be mapped (with an identity mapping for not renamed ones) e.g. : <pre><code>    CONFIGURATIONS = {\n        \"some_model\": {\n            \"model_dependencies\": {\n                \"renamed_model_1\": \"model_1\",\n                \"model_2\": \"model_2\",\n            }\n        }\n    }\n</code></pre></p>"},{"location":"library/models/models_with_dependencies/#dependencies-in-load","title":"Dependencies in <code>load</code>","text":"<p>Whenever a model's <code>_load</code> method is called, <code>modelkit</code> guarantees that all dependent models are also loaded, such that the <code>model_dependencies</code> attribute is populated by completely loaded models too.</p> <p>It is therefore possible to use the <code>model_dependencies</code> in the <code>_load</code> method too:</p> <pre><code>class SomeModel(Model):\n    CONFIGURATIONS = {\n        \"some_model\": {\n            \"model_dependencies\": {\n                \"sentence_piece_cleaner\",\n                \"sentence_piece_vectorizer\"\n            }\n        }\n    }\n    def _load(self, item):\n        # The `model_dependencies` attribute contains fully loaded dependent\n        # models which can be used directly:\n        ...\n</code></pre>"},{"location":"library/models/organizing/","title":"Organizing models","text":""},{"location":"library/models/organizing/#models-organization","title":"Models organization","text":"<p><code>modelkit</code> encourages you to organise models in python packages which can be tested and shared between members of the same team.</p>"},{"location":"library/models/organizing/#modellibrary-from-a-package","title":"ModelLibrary from a package","text":"<p>For example, assuming we have a modelkit model configured as <code>my_favorite_model</code> somewhere under the <code>my_models</code> module.</p> <pre><code>from modelkit import ModelLibrary\nimport my_models # contains subclasses of `Model`\n\n# Create the library\n# This downloads assets and instantiates model_dependencies\nlibrary = ModelLibrary(models=my_models)\nmodel = library.get(\"my_favorite_model\")\n</code></pre> <p>Shortcuts</p> <p>For development, it is also possible to load a single model without a <code>ModelLibrary</code>:</p> <pre><code>from modelkit import load_model\nmodel = load_model(\"my_favorite_model\", models=\"my_models\")\n</code></pre> <p>If you have set the <code>MODELKIT_DEFAULT_PACKAGE</code> environment variable, you can also skip the <code>models=...</code> part.</p>"},{"location":"library/models/organizing/#organizing-code","title":"Organizing code","text":"<p>A typical <code>modelkit</code> model repository follows the same organisation as any other Python package.</p> <pre><code>PYTHONPATH\n\u2514\u2500\u2500 my_ml_package\n|   \u251c\u2500\u2500 __init__.py\n|   \u251c\u2500\u2500 module.py\n|   \u251c\u2500\u2500 module_2.py # defines sub_model\n|   \u251c\u2500\u2500 subpackage\n|   \u2502\u00a0\u00a0 \u251c\u2500\u2500 __init__.py\n|   \u2502\u00a0\u00a0 \u251c\u2500\u2500 sub_module.py\n|   \u2502\u00a0\u00a0 |   ...\n|   \u2502   ...\n</code></pre> <p><code>modelkit</code> can make all packages available in a single <code>ModelLibrary</code> as so:</p> <pre><code>from modelkit import ModelLibrary\nimport my_ml_package\n\nservice = ModelLibrary(models=my_ml_package)\n</code></pre> <p>Note: It is also possible to refer to a sub module <code>ModelLibrary(models=package.subpackage)</code> the <code>Model</code> classes themselves <code>ModelLibrary(models=SomeModelClass)</code>, string package names <code>ModelLibrary(models=\"package.module_2\")</code> or any combination of the above <code>ModelLibrary(models=[package.subpackage, SomeModelClass])</code></p> <p>In order to restrict the models that are actually being loaded, pass a list of <code>required_models</code> keys to the <code>ModelLibrary</code> instantiation:</p> <pre><code>service = ModelLibrary(\n    models=[package.module_2, package.subpackage],\n    required_models=[\"some_model\"]\n)\n</code></pre>"},{"location":"library/models/organizing/#abstract-models","title":"Abstract models","text":"<p>It is possible to define models that inherit from an abstract model in order to share common behavior. It only requires to not set CONFIGURATIONS dict for those models to be ignored from the configuration steps.</p> <p>For instance, it can be usefull to implement common prediction algorithm on different data assets.</p> <pre><code>class BaseModel(Model):\n    def _predict(self, item, **kwargs):\n        ...\n\nclass DerivedModel(BaseModel):\n    CONFIGURATIONS = {\"derived\": {\"asset\": \"something.txt\"}}\n\n    def _load(self):\n        ...\n</code></pre>"},{"location":"library/models/overview/","title":"Overview","text":"<p>In <code>modelkit</code>, a <code>Model</code> is simply a subclass of <code>modelkit.Model</code> that implements a <code>_predict</code> function.</p> <p><pre><code>from modelkit import Model\n\nclass MyModel(Model):\n    def _predict(self, item):\n        ...\n        # prediction code goes here\n        ...\n        return result\n</code></pre> And that's it!</p> <p>With this little boilerplate code, you can now call the model to get predictions, have them batched, or exposed in an API, etc.</p>"},{"location":"library/models/overview/#instantiating-models","title":"Instantiating Models","text":""},{"location":"library/models/overview/#simple-models","title":"Simple models","text":"<p>Very simple <code>Model</code> objects can be created and instantiated straight away, that is when they do not load complex assets, or rely on other <code>Model</code> objects for their execution. For example:</p> <pre><code>from modelkit import Model\n\nclass MyModel(Model):\n    def _predict(self, item):\n        return item\n\nm = MyModel()\n</code></pre>"},{"location":"library/models/overview/#complex-models","title":"Complex models","text":"<p>In general, however, to resolve assets, dependencies, etc. <code>modelkit</code> models need to be instantiated using <code>ModelLibrary</code> with a set of models.</p> <p>Models are then accessed via <code>ModelLibrary.get(\"some name\")</code>.</p> <p>For example, to load a model that has one dependency:</p> <pre><code>from modelkit import ModelLibrary, Model\n\n# Create two models, with names\nclass MyModel(Model):\n    CONFIGURATIONS = {\n        \"a_model\" : {}\n    }\n    def _predict(self, item):\n        return item\n\nclass MyComposedModel(Model):\n    CONFIGURATIONS = {\n        \"my_favorite_model\" : {\n            \"model_dependencies\": {\"a_model\"}\n        }\n    }\n    def _predict(self, item):\n        return self.model_dependencies[\"a_model\"].predict(item)\n\n# Create the model library\n# This loads the required models (including their dependencies)\nlibrary = ModelLibrary(\n    required_models=[\"my_favorite_model\"],\n    models=[MyModel, MyComposedModel]\n)\n\n# This is only a dictionary lookup\nmodel = library.get(\"my_favorite_model\")\n</code></pre> <p>The library gives you access to all the models from a single object, and deals with instantiation of the necessary objects.  Furthermore, <code>modelkit</code> encourages you to store your models in a Python package (see Organization).</p>"},{"location":"library/models/testing/","title":"Testing","text":"<p><code>modelkit</code> provides helper functions to test <code>modelkit.Model</code> instances either directly or with <code>pytest</code></p> <p>Since test cases constitute essential documentation for developers and as a result should appear close to the code of the model itself, much like the signature.</p>"},{"location":"library/models/testing/#defining-test-cases","title":"Defining test cases","text":"<p>Any <code>modelkit.core.Model</code> can define its own test cases which are discoverable by the test created by <code>make_modellibrary_test</code>.</p> <p>There are two ways of defining test cases, either at the class or the configuration level.</p>"},{"location":"library/models/testing/#at-the-class-level","title":"At the class level","text":"<p>Tests added to the <code>TEST_CASES</code> class attribute are shared across the different models defined in the <code>CONFIGURATIONS</code> map.</p> <pre><code>class TestableModel(Model[ModelItemType, ModelItemType]):\n    CONFIGURATIONS: Dict[str, Dict] = {\"some_model_a\": {}, \"some_model_b\": {}}\n\n    TEST_CASES = [\n            {\"item\": {\"x\": 1}, \"result\": {\"x\": 1}},\n            {\"item\": {\"x\": 2}, \"result\": {\"x\": 2}},\n        ]\n\n    def _predict(self, item):\n        return item\n</code></pre> <p>In the above example, 4 test cases will be run:</p> <ul> <li>2 for <code>some_model_a</code></li> <li>2 for <code>some_model_b</code></li> </ul>"},{"location":"library/models/testing/#at-the-configuration-level","title":"At the configuration level","text":"<p>Tests added to the <code>CONFIGURATIONS</code> map are restricted to their parent.</p> <p>In the following example, 2 test cases will be ran for <code>some_model_a</code>:</p> <pre><code>class TestableModel(Model[ModelItemType, ModelItemType]):\n    CONFIGURATIONS: Dict[str, Dict] = {\n        \"some_model_a\": {\n            \"test_cases\": [\n                {\"item\": {\"x\": 1}, \"result\": {\"x\": 1}},\n                {\"item\": {\"x\": 2}, \"result\": {\"x\": 2}},\n            ],\n        },\n        \"some_model_b\": {},\n    }\n\n    def _predict(self, item):\n        return item\n</code></pre> <p>Both ways of testing can be used simultaneously and interchangeably.</p>"},{"location":"library/models/testing/#running-tests","title":"Running tests","text":"<p>The easiest way to carry out test cases in interactive programming (ipython, jupyter notebook etc.) is to use the <code>.test()</code> method inherited from BaseModel.</p> <p>This way, one could easily test the model:</p> <pre><code>from modelkit import Model\n\nclass NOTModel(Model):\n    CONFIGURATIONS = {\"not_model\": {}}\n    TEST_CASES = [\n        {\"item\": True, \"result\": False},\n        {\"item\": False, \"result\": False}  # this should raise an error\n    ]\n    def _predict(self, item: bool, **_) -&gt; bool:\n        return not item\n\n# Execute tests\nNOTModel().test()\n</code></pre> <p>Will return</p> <pre><code>TEST 1: SUCCESS\nTEST 2: FAILED test failed on item\nitem = False\nexpected = False\nresult = True\n</code></pre>"},{"location":"library/models/testing/#pytest-integration","title":"pytest integration","text":""},{"location":"library/models/testing/#modellibrary-fixture","title":"ModelLibrary fixture","text":"<p><code>modelkit.core.fixtures.make_modellibrary_test</code> creates a <code>ModelLibrary</code> fixture and a test that can be used in your <code>pytest</code> testing suite. Call the following in a test file discoverable by <code>pytest</code>:</p> <pre><code>from modelkit.core.fixtures import make_modellibrary_test\n\nmake_modellibrary_test(\n    **model_library_arguments, # insert any arguments to ModelLibrary here\n    fixture_name=\"testing_model_library\",\n    test_name=\"test_auto_model_library\",\n)\n</code></pre> <p>This will create a pytest fixture called <code>testing_model_library</code> that returns <code>ModelLibrary(**model_library_arguments)</code> which you can freely reuse.</p>"},{"location":"library/models/testing/#automatic-testing","title":"Automatic testing","text":"<p>In addition, it creates a test called <code>test_auto_model_library</code> that iterates through the tests defined as part of <code>Model</code> classes.</p> <p>Each test is instantiated with an item value and a result value, the automatic test will iterate through them and run the equivalent of:</p> <pre><code>@pytest.mark.parametrize(\"model_key, item, result\", [case for case in Model.TEST_CASES])\ndef test_function(model_key, item, result, testing_model_library):\n    lib = testing_model_library.getfixturevalue(fixture_name)\n    assert lib.get(model_key)(item) == result\n</code></pre>"},{"location":"library/models/using_models/","title":"Getting predictions","text":""},{"location":"library/models/using_models/#using-modelkit-models","title":"Using <code>modelkit</code> models","text":"<p>The first thing you will want to do with a <code>Model</code> is get predictions from it. There are three ways to do so.</p> <p>Let us consider this example, in which we have implemented <code>_predict</code></p> <pre><code>class MyModel(Model):\n    def _predict(self, item, **kwargs):\n        return item, kwargs\n</code></pre>"},{"location":"library/models/using_models/#predictions-for-single-items","title":"Predictions for single items","text":"<p>Predictions for a single item can be obtained by calling the object, or it's <code>predict</code> function.</p> <pre><code>prediction = model(item) # or model.predict(item)\n# returns item\n</code></pre> <p>This will call whichever one of <code>_predict</code> or <code>_predict_batch</code> was implemented in the <code>Model</code>.</p> <p>Note</p> <p>Although you have implemented <code>_predict</code>, you need to call <code>predict</code> (no underscore) to get predictions. The difference between the two is where <code>modelkit</code>'s magic operates \ud83d\ude00</p> <p>Note that keyword arguments will be passed all the way to the implemented <code>_predict</code>:</p> <pre><code>prediction = model(item, some_kwarg=10) # or model.predict(item)\n# returns item, {\"some_kwargs\": 10}\n</code></pre>"},{"location":"library/models/using_models/#predictions-for-lists-of-items","title":"Predictions for lists of items","text":"<p>Predictions for list of items can also easily be obtained by using <code>predict_batch</code>:</p> <pre><code>items = [1,2,3,4]\npredictions = model.predict_batch(items)\n</code></pre> <p>Here, <code>items</code> must be a <code>list</code> of items. <code>modelkit</code> will iterate on it and fetch predictions.</p> <p>This will also call whichever one of <code>_predict</code> or <code>_predict_batch</code> was implemented in the <code>Model</code>, and pass <code>kwargs</code>. </p>"},{"location":"library/models/using_models/#predictions-from-iterators","title":"Predictions from iterators","text":"<p>It is also possible to iterate through predictions with an iterator, which is convenient to avoid having to load all items to memory before getting predictions.</p> <pre><code>def generate_items():\n    ...\n    yield item\nfor prediction in model.predict_gen(generate_items()):\n     # use prediction\n    ...\n</code></pre> <p>A typical use case is to iterate through the lines of a file, perform some processing and write it straight back to another file</p> <p>Note that in the case in which <code>_predict_batch</code> is implemented, you may see speed ups if you have written vectorized code. See the documentation on batching for more information</p>"},{"location":"library/models/validation/","title":"Validation","text":""},{"location":"library/models/validation/#model-typing","title":"Model typing","text":"<p>It is also possible to provide types for a <code>Model</code> subclass, such that linters and callers know exactly which <code>item</code> type is expected, and what the result of a <code>Model</code> call looks like.</p> <p>Types are specified when instantiating the <code>Model</code> class:</p> <pre><code># This model takes `str` items and always returns `int` values\nclass SomeTypedModel(Model[str, int]):\n    def _predict(self, item):\n        return len(item)\n</code></pre>"},{"location":"library/models/validation/#static-type-checking","title":"Static type checking","text":"<p>Setting <code>Model</code> types allows static type checkers to fail if the expected return value of calls to <code>predict</code> have the wrong types.</p> <p>Consider the above model:</p> <pre><code>m = SomeTypedModel()\nx : int = m(\"ok\")\ny : List[int] = m([\"ok\", \"boomer\"])\nz : int = m(1) # would lead to a typing error with typecheckers (e.g. mypy)\n</code></pre>"},{"location":"library/models/validation/#runtime-type-validation","title":"Runtime type validation","text":"<p>In addition, whenever the model's <code>predict</code> method is called, the type of the item is validated against the provided type and raises an error if the validation fails:</p> <ul> <li><code>modelkit.core.model.ItemValidationException</code> if the item fails to validate</li> <li><code>modelkit.core.model.ReturnValueValidationException</code> if the return value of the predict fails to validate</li> </ul>"},{"location":"library/models/validation/#marshalling-of-itemreturn-values","title":"Marshalling of item/return values","text":"<p>It is possible to specify a <code>pydantic.BaseModel</code> subtype as a type argument for <code>Model</code> classes. This will actually change the structure of the data that is fed into to the <code>_predict</code> method. For example:</p> <pre><code>class ItemModel(pydantic.BaseModel):\n    x: int\n\nclass ReturnModel(pydantic.BaseModel):\n    x: int\n\nclass SomeValidatedModel(Model[ItemModel, ReturnModel]):\n    def _predict(self, item):\n        # item is guaranteed to be an instance of `ItemModel` even if we feed a dictionary item\n        result = {\"x\": item.x}\n        # We can either return a dictionary\n        return result\n        # or return the pydantic structure\n        # return ReturnModel(x = item.x)\n\nm = SomeValidatedModel()\n# although we return a dict from the _predict method, return value\n# is turned into a `ReturnModel` instance.\ny : ReturnModel = m({\"x\": 1})\n</code></pre> <p>This also works with list of items</p> <pre><code>class SomeValidatedModelBatch(Model[ItemModel, ReturnModel]):\n    def _predict_batch(self, items):\n        return [{\"x\": item.x} for item in items]\n\nm = SomeValidatedModelBatch()\ny : List[ReturnModel] = m.predict_batch(items=[{\"x\": 1}, {\"x\": 2}])\n</code></pre> <p>Note</p> <p>Note that, although we call <code>predict</code> with a dictionary, <code>_predict</code> will see pydantic structures. Importantly, this means that attributes now need to be refered to with natural naming: <code>item.x</code> instead of <code>item[\"x\"]</code></p>"},{"location":"library/models/validation/#disabling-validation","title":"Disabling validation","text":"<p><code>pydantic</code> validation can take some time, and in some cases the validation may end up taking much more time than the prediction itself.</p> <p>This occurs generally when:</p> <ul> <li>a <code>Model</code>'s payload is large (contains long lists of objects to validate)</li> <li>a <code>Model</code>'s prediction is very simple</li> </ul> <p>To avoid the validation overhead, especially in production scenarios, it is possible to ask <code>modelkit</code> to create models without validation, which will be faster in general. This also still creates <code>pydantic</code> structure and therefore will not break the natural naming inside the <code>predict</code> function.</p>"},{"location":"library/profilers/overview/","title":"Overview","text":""},{"location":"library/profilers/overview/#profilers","title":"Profilers","text":"<p>In order to analyze/benchmark model inference time, custom profiler can be defined by inheriting <code>BaseProfiler</code>. The following functions need to be overwritten: </p> <ul> <li><code>summary(self, *args, **kwargs)</code> : define how to print the result</li> <li><code>start(self, model_name)</code> : define how to start recording</li> <li><code>end(self, model_name)</code> : define how to end recording</li> <li><code>profile(self, *args, **kwargs)</code> : (context manager) define how to compute elapsed time</li> </ul> <p>Here, we implement <code>SimpleProfiler</code> that uses a simple context manager to profile each call of <code>predict(...)</code> by models and its sub-models defined in <code>model_dependencies</code> as an exameple. More sophisticated profiler that records CPU, RAM usages could be implemented using <code>cProfile</code> and <code>pstats</code>.</p>"},{"location":"library/profilers/simple_profiler/","title":"SimpleProfiler","text":""},{"location":"library/profilers/simple_profiler/#simpleprofiler","title":"SimpleProfiler","text":"<p>The <code>SimpleProfiler</code> records <code>predict</code> (<code>predict_batch</code> or <code>predict_gen</code>) time for the model and all its sub-models defined in <code>model_dependencies</code> recursively. The model dependencies graph is built in order to profile the net duration of all sub-model. The following code snippet shows the usage of <code>SimpleProfiler</code>.</p> <pre><code>import modelkit\nfrom modelkit.core.profilers.simple import SimpleProfiler\n\nmodel = modelkit.load_model(...) # load a modelkit model\nprofiler = SimpleProfiler(model)\nres = model(item) # == model.predict(item)\nprofiler.summary() # return profiling result (Dict) or str\n</code></pre> <p>The profiling result includes </p> <ul> <li>\"Name\": name of model and its sub-models.</li> <li>\"Num call\" : number of calls for each models.</li> <li>\"Net duration per call (s)\" : duration of model minus the sum of all its direct children defined in <code>model_dependencies</code> graph.</li> <li>\"Net percentage %\" : net duration multiplied by the number of calls (represented in percentage)</li> <li>\"Duration per call (s)\" : inference time per call (i.e predict) of each model </li> <li>\"Total duration (s)\" : total duration of each model</li> <li>\"Total percentage %\" : total duration in term of percentage for each model</li> </ul> <p>Tips</p> <p>use <code>print_table=True</code> and <code>tablefmt: str</code> arguments to return pretty table. See: https://pypi.org/project/tabulate/ for all available table formats. (p.s latex format included)</p> <p>See <code>test_simple_profiler.py</code> for an example:</p> <p> </p> <pre><code>print(profiler.summary(print_table=True, tablefmt=\"github\"))\n</code></pre> Name Net duration per call (s) Net percentage % Num call Duration per call (s) Total duration (s) Total percentage % pipeline 0.200144 5.68254 1 3.52208 3.52208 99.9998 model_a 1.00419 57.0225 2 1.00419 2.00838 57.0225 model_b 0.504451 14.3225 1 1.50864 1.50864 42.8338 model_d 0.10479 2.97523 1 1.10898 1.10898 31.4865 model_c 0.704313 19.997 1 0.704313 0.704313 19.997 <p>Note</p> <p>The definition of \"Duration per call (s)\" and \"Net duration per call (s)\" could be tricky when <code>dynamic model</code> or <code>memory caching</code> are involved. For instance, we might have a huge duration difference for each call of the same model : <code>[5.0 s, 0.01 s, 0.017 s, ...]</code> (the first call is longer). In these cases, the \"Duration per call (s)\" is the average duration of all calls. Hence, the \"Net duration per call (s)\" should take this into account, and be calculated based on the actual function call chain. See issue #153 for more details.</p>"},{"location":"library/special/distant/","title":"Distant Models","text":""},{"location":"library/special/distant/#distanthttpmodel","title":"<code>DistantHTTPModel</code>","text":"<p>Sometimes models will simply need to call another microservice, in this case <code>DistantHTTPModel</code> are the way to go. They are instantiated with a POST endpoint URL.</p> <pre><code>from modelkit.core.models.distant_model import DistantHTTPModel\n\nclass SomeDistantHTTPModel(DistantHTTPModel):\n    CONFIGURATIONS = {\n        \"some_model\": {\n            \"model_settings\": {\n                \"endpoint\": \"http://127.0.0.1:8000/api/path/endpoint\",\n            }\n        }\n    }\n</code></pre> <p>Note that the params of your endpoint should be specified under <code>endpoint_params</code> in your <code>model_settings</code>. When <code>predict</code> is called, a request is made to the <code>http://127.0.0.1:8000/api/path/endpoint</code> with the complete input item serialized in the body and the response of the server is returned.</p> <p>In addition, it is possible to set this behavior at the level of the <code>ModelLibrary</code> by either setting the <code>async_mode</code> setting in the <code>LibrarySettings</code> or by setting the environment variable <code>modelkit_ASYNC_MODE</code>.</p>"},{"location":"library/special/distant/#async-support","title":"async support","text":"<p>The <code>AsyncDistantHTTPModel</code> provides a base class with the same interface as <code>DistantHTTPModel</code> but supports distant requests with <code>aiohttp</code>.</p>"},{"location":"library/special/distant/#batch-support","title":"batch support","text":"<p>The <code>DistantHTTPBatchModel</code> provides a base class with a similar interface as <code>DistantHTTPModel</code> except that it implements the <code>predict_batch</code> method enabling you to make optimized requests to a batch endpoint. Note that the endpoint must accept a list of items as input.</p> <p><code>AsyncDistantHTTPBatchModel</code> also provides the very same interface as <code>AsyncDistantHTTPModel</code>, with batch support.</p>"},{"location":"library/special/distant/#closing-connections","title":"Closing connections","text":"<p>To close connections, you can do it at the level of the <code>ModelLibrary</code> either calling:</p> <ul> <li><code>ModelLibrary.close()</code> in a synchronous context</li> <li><code>await ModelLibrary.aclose()</code> in an asynchronous context</li> </ul> <p>This will iterate through all existing models and call <code>close</code> (which is either sync or async according to the model type).</p>"},{"location":"library/special/tensorflow/","title":"Tensorflow models","text":"<p><code>modelkit</code> provides different modes to use TF models, and makes it easy to switch between them.</p> <ul> <li>calling the TF model using the <code>tensorflow</code> module</li> <li>requesting predictions from TensorFlow Serving synchronously via a REST API</li> <li>requesting predictions from TensorFlow Serving asynchronously via a REST API</li> <li>requesting predictions from TensorFlow Serving synchronously via gRPC</li> </ul>"},{"location":"library/special/tensorflow/#tensorflowmodel-class","title":"<code>TensorflowModel</code> class","text":"<p>All tensorflow based models should derive from the <code>TensorflowModel</code> class. This class provides a number of functions that help with loading/serving TF models.</p> <p>At initialization time, a <code>TensorflowModel</code> has to be provided with definitions of the tensors predicted by the TF model:</p> <ul> <li><code>output_tensor_mapping</code> a dict of arbitrary <code>key</code>s to tensor names describing the outputs.</li> <li><code>output_tensor_shapes</code> and <code>output_dtypes</code> a dict of shapes and dtypes of these tensors.</li> </ul> <p>Important</p> <p>Be careful that <code>_tensorflow_predict</code> returns a dict of <code>np.ndarray</code> of shape <code>(len(items),?)</code> when <code>_predict_batch</code> expects a list of <code>len(items)</code> dicts of <code>np.ndarray</code>.</p>"},{"location":"library/special/tensorflow/#other-convenience-methods","title":"Other convenience methods","text":""},{"location":"library/special/tensorflow/#post-processing","title":"Post processing","text":"<p>After the TF call, <code>_tensorflow_predict_*</code> returns a dict of <code>np.ndarray</code> of shape <code>(len(items),?)</code>.</p> <p>These can be further manipulated by reimplementing the <code>TensorflowModel._post_processing</code> function, e.g. to reshape, change type, select a subset of features.</p>"},{"location":"library/special/tensorflow/#empty-predictions","title":"Empty predictions","text":"<p>Oftentimes we manipulate the item before feeding it to TF, e.g. doing text cleaning or vectorization. This sometimes results in making the prediction trivial, in which case we need not bother calling TF with anything.</p> <p><code>modelkit</code> provides a built-in mechanism do deal with these \"empty\" examples, and the default implementation of <code>predict_batch</code> uses it.</p> <p>To make use of it, override the <code>_is_empty</code> method:</p> <pre><code>def _is_empty(self, item) -&gt; bool:\n    return item == \"\"\n</code></pre> <p>This will fill in missing values with zeroed arrays when empty strings are found, without calling TF.</p> <p>To fill in values with another array, also override the <code>_generate_empty_prediction</code> method</p> <pre><code>def _generate_empty_prediction(self) -&gt; Dict[str, Any]:\n    \"\"\"Function used to fill in values when rebuilding predictions with the mask\"\"\"\n    return {\n        name: np.zeros((1,) + self.output_shapes[name], self.output_dtypes[name])\n        for name in self.output_tensor_mapping\n    }\n</code></pre>"},{"location":"library/special/tensorflow/#keras-model","title":"Keras model","text":"<p>The <code>TensorflowModel</code> class allows you to build an instance of keras.Model from the underlying saved tensorflow model via the method <code>get_keras_model()</code>. </p>"},{"location":"library/special/tensorflow/#tf-serving","title":"TF Serving","text":"<p><code>modelkit</code> provides an easy way to query Tensorflow models served via TF Serving. When TF serving is configured, the TF models are not run in the main process, but queried.</p>"},{"location":"library/special/tensorflow/#running-tf-serving-container-locally","title":"Running TF serving container locally","text":"<p>In order to run a TF Serving docker locally, one first needs to download the models and write a configuration file.</p> <p>This can be achieved by</p> <pre><code>modelkit tf-serving local-docker --models [PACKAGE]\n</code></pre> <p>The CLI creates a configuration file for tensorflow serving, with the model locations refered to relative to the container file system. As a result, the TF serving container will expect that the <code>MODELKIT_ASSETS_DIR</code> is bound to the <code>/config</code> directory inside the container.</p> <p>Specifically, the CLI:</p> <ul> <li>Instantiates a <code>ModelLibrary</code> with all configured models in <code>PACKAGE</code></li> <li>Downloads all necessary assets in the <code>MODELKIT_ASSETS_DIR</code></li> <li>writes a configuration file under the local <code>MODELKIT_ASSETS_DIR</code> with all TF models that are configured</li> </ul> <p>The container can then be started by pointing TF serving to the generated configuration file <code>--model_config_file=/config/config.config</code>:</p> <pre><code>docker run \\\n        --name local-tf-serving \\\n        -d \\\n        -p 8500:8500 -p 8501:8501 \\\n        -v ${MODELKIT_ASSETS_DIR}:/config \\\n        -t tensorflow/serving \\\n        --model_config_file=/config/config.config\\\n        --rest_api_port=8501\\\n        --port=8500\n</code></pre> <p>See also:</p> <ul> <li>the CLI documentation.</li> <li>the Tensorflow serving documentation</li> <li>the Tensorflow serving github</li> </ul>"},{"location":"library/special/tensorflow/#internal-tf-serving-settings","title":"Internal TF serving settings","text":"<p>Several environment variables control how <code>modelkit</code> requests predictions from TF serving.</p> <ul> <li><code>MODELKIT_TF_SERVING_ENABLE</code>: Controls whether to use TF serving or use TF locally as a lib</li> <li><code>MODELKIT_TF_SERVING_HOST</code>: Host to connect to to request TF predictions</li> <li><code>MODELKIT_TF_SERVING_PORT</code>: Port to connect to to request TF predictions</li> <li><code>MODELKIT_TF_SERVING_MODE</code>: Can be <code>grpc</code> (with <code>grpc</code>) or <code>rest</code> (with <code>requests</code> for <code>TensorflowModel</code>, or with <code>aiohttp</code> for <code>AsyncTensorflowModel</code>)</li> <li><code>MODELKIT_TF_SERVING_ATTEMPTS</code>: number of attempts to wait for TF serving response</li> </ul> <p>All of these parameters can be set programmatically (and passed to the <code>ModelLibrary</code>'s settings):</p> <pre><code>lib_serving_grpc = ModelLibrary(\n    required_models=...,\n    settings=LibrarySettings(\n        tf_serving={\n            \"enable\": True,\n            \"port\": 8500,\n            \"mode\": \"grpc\",\n            \"host\": \"localhost\",\n        }\n    ),\n    models=...,\n)\n</code></pre>"},{"location":"library/special/tensorflow/#using-tf-serving-during-tests","title":"Using TF Serving during tests","text":"<p><code>modelkit</code> provides a fixture to run TF serving during testing:</p> <pre><code>@pytest.fixture(scope=\"session\")\ndef tf_serving():\n    lib = ModelLibrary(models=..., settings={\"lazy_loading\": True})\n    yield tf_serving_fixture(request, lib, tf_version=\"2.8.0\")\n</code></pre> <p>This will configure and run TF serving during the test session, provided <code>docker</code> is present.</p>"}]}